{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: ENSO Analysis\n",
    "\n",
    "Developed By: Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "Date: April 2024\n",
    "\n",
    "Requirements: list space, RAM, and pacakge requirements\n",
    "\n",
    "Link: notebook available to download at "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Description </u>\n",
    "\n",
    "This notebook helps the learner build intermediate python programming skills through data query, manipulation, analysis, and visualization. Learning will be centered around the El Nino Southern Oscillation (ENSO) climate pattern and its effects on temperature and precipitation. The notebook is aimed at learners who already have some knowledge of programming and statistics. \n",
    "\n",
    "<u> Summary of Contents </u>\n",
    "\n",
    "put an outline of tasks/skills here\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ENSO\n",
    "\n",
    "Put a description of what they are\n",
    "\n",
    "Include a bunch of links\n",
    "different ENSO indices https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Questions\n",
    "\n",
    "To pick up some useful intermediate python programming skills, this notebook will investigate the following ENSO-related science questions using simple statistics:\n",
    "\n",
    "1) How many seasons (boreal DJF,MAM,JJA,SON) since xxxx have had strong El Nino and La Nina conditions?\n",
    "2) Using composite analysis, what pattern do we see in sea surface temperature during El Nino and La Nina?\n",
    "3) In which season(s) does El Nino have the strongest impacts on global temperature and precipitation?\n",
    "4) Which areas of the United States experience statistically significant ENSO impacts on temperature and precipitation?\n",
    "5) On average, is it El Nino or La Nina that has stronger effects on temperature and precipitation in the US?\n",
    "\n",
    "**Disclaimer:** This notebook is intended for python programming learning. There are many datasets and statistical methods we could use to answer our science questions. The techniques used in this notebook are chosen for their simplicity since we are focused on learning intermediate programming skills as opposed to a focus on producing peer-review level analyses. You will undoubtedly see different techniques, thresholds, seasons, and more complex statisical methods used in ENSO literature. \n",
    "\n",
    "\n",
    "data description | frequency | units | dataset name | source\n",
    "---|---|---|---|---\n",
    "nino 3.4 sst index | monthly | n/a | Nino 3.4 SST index | [NOAA PSL](https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/nino34.long.anom.data)\n",
    "sea surface temperature | monthly | C | HadISST1 | [UKMO Hadley Centre](https://www.metoffice.gov.uk/hadobs/hadisst/)\n",
    "average air temperature | monthly | C | BEST | [Berkeley Earth](https://berkeleyearth.org/data/)\n",
    "precipitation | monthly | mm/day | NOAA PREC/L | [NOAA PSL](https://psl.noaa.gov/data/gridded/data.precl.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python Packages and Defining Your Workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the python packages we will need here\n",
    "\n",
    "import os\n",
    "# from urllib.request import urlretrieve\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy.testing as npt\n",
    "# import warnings\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import OrderedDict\n",
    "# import gzip\n",
    "# import shutil\n",
    "\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learners need to update these paths to reflect locations on their own computer/workspace\n",
    "\n",
    "# path to your working directory (where this notebook is on your computer)\n",
    "work_dir = r'C://Users/kerrie/Documents/01_LocalCode/repos/MSU_py_training/learn_by_doing/ENSO/' \n",
    "# work_dir = r'C://Users/kerrie.WIN/Documents/code/MSU_py_training/learn_by_doing/ENSO/' \n",
    "\n",
    "# path to where you'll download and store the data files\n",
    "data_dir = r'C://Users/kerrie/Documents/02_LocalData/tutorials/ENSO/'\n",
    "# data_dir=r'C://Users/kerrie.WIN/Documents/code/MSU_py_training/learn_by_doing/ENSO/'\n",
    "\n",
    "# path to write output files and figures\n",
    "output_dir = work_dir+'outputs/'\n",
    "\n",
    "# create directories if they don't exist already\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Data\n",
    "\n",
    "Scripted downloads of the datasets used here can be found in a separate notebook called [get_enso_datasets.ipynb](). If you haven't obtained the data already, use the get_enso_datasets notebook to download the Nino3.4 index, HadISST1 sea surface temperature, Berkeley Earth temperature, and GPCC precipitation data.\n",
    "\n",
    "# Data Pre-processing\n",
    "\n",
    "Set our 4 different datasets up with the same time dimension labels and calculate anomalies for SST, PR, and T using the same base period as the nino 3.4 index (1981-2010).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames\n",
    "nino_f = data_dir+'nino34_anomalies_monthly_NOAA.txt'\n",
    "sst_f = data_dir+'sst_monthly_HadISST1_UKMO.nc'\n",
    "t_f = data_dir+'tavg_monthly_BerkeleyEarth.nc'\n",
    "pr_f = data_dir+'precip_monthly_PRECL_NOAA.nc'\n",
    "\n",
    "# subset years\n",
    "year_start = '1948'\n",
    "year_end = '2023'\n",
    "\n",
    "# base period years (for anomalies)\n",
    "base_start = '1981'\n",
    "base_end = '2010'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nino 3.4 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nino3.4 index data\n",
    "\n",
    "# our data file contains a row for each year of data and each column is one of 12 monthly anomaly values for the Nino 3.4 area \n",
    "# the base period for the anomalies is 1981-2010\n",
    "\n",
    "# there are plenty of ways to load txt data, we'll use pandas\n",
    "nino_raw=pd.read_csv(nino_f,sep='\\s+',skiprows=1,skipfooter=7,header=None,index_col=0,na_values=-99.99,engine='python')\n",
    "# nino_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse the data into a 1D array timeseries\n",
    "nino=nino_raw.to_numpy().flatten()\n",
    "\n",
    "# len(nino),nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetimes\n",
    "dates=pd.date_range('1870-01-01','2024-12-01',freq='MS')\n",
    "\n",
    "# len(dates),dates[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an xarray object with metadata labels attached (time)\n",
    "nino=xr.DataArray(nino,dims='time',coords={'time':dates})\n",
    "# nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset in time using time labels\n",
    "nino=nino.sel(time=slice(year_start,year_end))\n",
    "# nino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea Surface Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "ds=xr.open_dataset(sst_f)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull variable from xr dataset\n",
    "sst=ds.sst\n",
    "# sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset in time\n",
    "\n",
    "# first assign new time values that will match nino (month start not center of months)\n",
    "dates=pd.date_range('1870-01-01','2024-02-01',freq='MS')\n",
    "sst['time']=dates\n",
    "\n",
    "# now subset in time\n",
    "sst=sst.sel(time=slice(year_start,year_end))\n",
    "# sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies\n",
    "\n",
    "# first calculate the monthly climatological values over the base period\n",
    "sst_base=sst.sel(time=slice(base_start,base_end))\n",
    "sst_clim=sst_base.groupby(sst_base.time.dt.month).mean('time')\n",
    "# sst_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the anomalies\n",
    "sst_anom=sst.groupby(sst.time.dt.month) - sst_clim\n",
    "# sst_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset(pr_f)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull variable from xr dataset\n",
    "pr=ds.precip\n",
    "\n",
    "# this data's times already match nino's so we don't need to re-assign the coordinate labels\n",
    "# just subset\n",
    "pr=pr.sel(time=slice(year_start,year_end))\n",
    "\n",
    "# calculate anomalies\n",
    "pr_base=pr.sel(time=slice(base_start,base_end))\n",
    "pr_clim=pr_base.groupby(pr_base.time.dt.month).mean('time')\n",
    "pr_anom=pr.groupby(sst.time.dt.month) - pr_clim\n",
    "\n",
    "# pr_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset(t_f)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these dates are whacky so we'll replace with datetimes to match the other datasets\n",
    "dates=pd.date_range('1750-01-01','2024-03-01',freq='MS')\n",
    "ds['time']=dates\n",
    "\n",
    "# we also need to rename the dimension 'month_number' for groupby to work correctly\n",
    "ds=ds.rename({'month_number':'month'})\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change base period\n",
    "# this data is provided as anomalies using the base period 1951-1980\n",
    "# we need to change the base period to match the rest of our data anomalies\n",
    "\n",
    "# pull variables from xr dataset\n",
    "t_anom_5180=ds.temperature\n",
    "clim_5180=ds.climatology\n",
    "\n",
    "# create temperature values working backward with anomalies plus climatology\n",
    "t=t_anom_5180.groupby(t_anom_5180.time.dt.month)+clim_5180\n",
    "\n",
    "# new base period climatological values\n",
    "t_base=t.sel(time=slice(base_start,base_end))\n",
    "clim_8110 = t_base.groupby(t_base.time.dt.month).mean('time')\n",
    "\n",
    "# anomalies with new base period\n",
    "t_anom=t.groupby(t.time.dt.month)-clim_8110\n",
    "\n",
    "# subset in time\n",
    "t_anom=t_anom.sel(time=slice(year_start,year_end))\n",
    "# t_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start our analysis we the variables nino, sst_anom, pr_anom, and t_anom. If you are used to seeing a list of variables you've made (like in Matlab or RStudo) that is available for python/Jupyter through the IDE you use (like VS Code). There is usually a console option that you can open to see all the variables you've made is point. This is useful for seeing variable shapes and data types as well as seeing which variables you could potentially delete if you suspect you'll be memory limited during your analysis.\n",
    "\n",
    "Let's double check our 4 variable shapes below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nino.shape, sst_anom.shape, pr_anom.shape, t_anom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds,nino_raw,pr,pr_base,pr_clim,sst,sst_base,sst_clim,t,t_anom_5180,t_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!\n",
    "\n",
    "### Choose one of three coding mini-projects below to complete on your own and prepare to share your findings\n",
    "\n",
    "\n",
    "**Option 1 (easiest):** \n",
    "\n",
    "&emsp;Hints:\n",
    "- \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Option 2 (moderate):** \n",
    "\n",
    "&emsp;Hints:\n",
    "- \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**Option 3 (hardest):**\n",
    "\n",
    "&emsp;Hints:\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at the answer figure for option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at the answer figure for option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at the answer figure for option 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to create answer codes for these and put them in the repo. Direct learners to answers after the work-on-your-own session."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('tutorials_simple')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "957efa23bb835907639de5eb4a56f03f6ae7a80dfdfd0cd2554d336fbf8b9b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
