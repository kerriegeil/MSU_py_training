{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: Climate Change Indicators\n",
    "\n",
    "Developed By: Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "Date: January 2024\n",
    "\n",
    "Requirements: list space, RAM, and pacakge requirements\n",
    "\n",
    "Link: notebook available to download at "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Description </u>\n",
    "\n",
    "This notebook helps the learner build intermediate python programming skills through data query, manipulation, analysis, and visualization. Learning will be centered around obtaining climate data, computing climate change indices, and determining the statistical significance of change. The notebook is aimed at learners who already have some knowledge of programming and statistics. \n",
    "\n",
    "<u> Summary of Contents </u>\n",
    "\n",
    "put an outline of tasks/skills here\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Climate Change Indicators\n",
    "\n",
    "Put a description of what they are\n",
    "\n",
    "Include a bunch of links\n",
    "\n",
    "Spell out which ones we will be computing\n",
    "\n",
    "Selection of ETCCDI Climate Extremes Indices\n",
    "- Monthly Maximum Value of Daily Minimum Temperature (TNx)\n",
    "- Growing Season Length (GSL)\n",
    "- Warm Spell Duration Index (WSDI)\n",
    "- Monthly Maximum Consecutive 5-day Precipitation (Rx5day)\n",
    "- Maximum Length of Consecutive Dry Days (CDD)\n",
    "- Annual Total Precip Amount Over 99th Percentile of Wet Days (R99pTOT)\n",
    "\n",
    "**Disclaimer:** This notebook is intended for python programming learning only. The data quality checking and calculation of ETCCDI climate change indices in this notebook may differ from the ETCCDI published instructions for simplicity and/or relevance to our learning goals. Learners wanting to compute the indices according to the exact ETCCDI instructions (which would be required for example for use in a scientific publication) should consult the [documentation](https://etccdi.pacificclimate.org/index.shtml). The documentation suggests using the [RClimDex software package](https://github.com/ECCC-CDAS/RClimDex.git) written in R to calculate ETCCDI climate change indices. Another option would be to use pre-calculated indices based on multiple gridded datasets available at [climdex.org](https://www.climdex.org/), which also offers a similar software package for calculating the indices on a dataset of your choice.   \n",
    "\n",
    "\n",
    "For the climate change indices covered in this notebook we will need the following observational data over many data years:\n",
    "\n",
    "variable abbrev. | description | frequency | units\n",
    "---|---|---|---\n",
    "tmin | minimum surface air temperature | daily | C \n",
    "tmax | maximum surface air temperature | daily | C\n",
    "prcp | accummulated precipitation | daily | mm/day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python Packages and Defining Your Workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the python packages we will need here\n",
    "\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy.testing as npt\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "# import gzip\n",
    "# import shutil\n",
    "\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learners need to update these paths to reflect locations on their own computer/workspace\n",
    "\n",
    "# path to your working directory (where this notebook is on your computer)\n",
    "work_dir = r'C://Users/kerrie/Documents/01_LocalCode/repos/MSU_py_training/learn_by_doing/climate_change_indicators/' \n",
    "# work_dir = r'C://Users/kerrie.WIN/Documents/code/MSU_py_training/learn_by_doing/climate_change_indicators/' \n",
    "\n",
    "# path to where you'll download and store the data files\n",
    "data_dir = r'C://Users/kerrie/Documents/02_LocalData/tutorials/AgERA5_daily/'\n",
    "# data_dir=r'C://Users/kerrie.WIN/Documents/data/AgERA5/'\n",
    "\n",
    "# path to write output files and figures\n",
    "output_dir = r'C://Users/kerrie/Documents/01_LocalCode/repos/MSU_py_training/learn_by_doing/climate_change_indicators/outputs/'\n",
    "# output_dir = r'C://Users/kerrie.WIN/Documents/code/MSU_py_training/learn_by_doing/climate_change_indicators/outputs/'\n",
    "\n",
    "\n",
    "# create directories if they don't exist already\n",
    "# if not os.path.exists(work_dir):\n",
    "#     os.makedirs(work_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Data\n",
    "\n",
    "We will use the AgERA5 gridded dataset in this notebook, obtained from the [Copernicus Climate Data Store (CDS)](https://cds.climate.copernicus.eu/cdsapp#!/dataset/sis-agrometeorological-indicators?tab=overview). Data file are temporarily available during the workshop at [John website here](). These files have been minimally pre-processed to save time during the workshop. We will download them together in subsequent notebook cells. \n",
    "\n",
    "The pre-processing steps that have been completed for you include:\n",
    "1) Using the Climate Data Store (CDS) API to download AgERA5 precipitation and 2m min and max temperature. You need a free CDS account and to install the cdsapi python package as well as python dask to do this. The data downloads as one .tar.gz file per year per variable.\n",
    "2) Unpacking the .tar.gz files. Each zipped archive unpacks to one netcdf file per day per variable (many files).\n",
    "3) Converting temperature units from K to C\n",
    "4) Converting longitude coordinates from 0 to 360 to -180 to 180\n",
    "5) Consolidating all the daily files into one single netcdf file per variable that contains all times.\n",
    "\n",
    "We won't cover this process here but there I provide the python scripts that document this process, if you are interested. Step 1-2 is performed in [get_AgERA5_daily_parallel.py]() and steps 3-5 are performed in [prep_AgERA5_daily.ipynb]().\n",
    "\n",
    "\n",
    "\n",
    "Describe the data requirements (importance of time dimension standardization and missing data) \n",
    "\n",
    "Warnings against performing climate change analyses on just any dataset (example PRISM)\n",
    "\n",
    "Warnings about high resolution spatial data (much of it is interpolated, high res not always better)\n",
    "\n",
    "Why we choose to use certain datasets\n",
    "\n",
    "Links to each dataset's webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = ['https://www.url-to-johns-data-page',\n",
    "#         'https://www.url-to-johns-data-page',\n",
    "#         'https://www.url-to-johns-data-page']\n",
    "\n",
    "filenames = [data_dir+'prcp_AgERA5_Starkville_Daily_1979-2023.nc',\n",
    "            data_dir+'tmax_AgERA5_Starkville_Daily_1979-2023.nc',\n",
    "            data_dir+'tmin_AgERA5_Starkville_Daily_1979-2023.nc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to python xarray data structures\n",
    "\n",
    "Add introductory info/exploration of xarray data structures and the attached metadata labels here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = xr.open_dataarray(filenames[0])\n",
    "tx = xr.open_dataarray(filenames[1])\n",
    "tn = xr.open_dataarray(filenames[2])\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=pr.squeeze()\n",
    "tx=tx.squeeze()\n",
    "tn=tn.squeeze()\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "### ETCCDI suggested data cleaning / quality control\n",
    "\n",
    "The minimum quality control procedures suggested by ETCCDI are as follows.\n",
    "\n",
    "Replace data value with Nan for:\n",
    "- user-defined missing values (i.e -9999-->Nan)\n",
    "- daily precip values less than 0\n",
    "- daily max temperature less than daily minimum temperature\n",
    "- daily temperature greater than 70C (158F) or less than -70C (-94F)\n",
    "- leap days (i.e Feb 29th)\n",
    "- impossible dates (i.e. 32nd March, 12th June 2042)\n",
    "- non-numeric values\n",
    "- daily temperature outliers (i.e. 3-5 times the standard deviation from the mean value for each calendar day)\n",
    "\n",
    "\n",
    "Addressing each of these items below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan for user-defined missing values (i.e -9999-->Nan)\n",
    "\n",
    "xr.open_dataset does this for you. \n",
    "\n",
    "\n",
    "Notice in the variable attributes above there is no _FillValue=-9999., which the value stored for missing data in the netcdf. This is because xarray automatically replaces the _FillValue with nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan for daily precip values less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any negatives?\n",
    "(pr<0).data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan for daily max temperature less than daily minimum temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is tx ever less than tn?\n",
    "(tx<tn).data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # where tx<tn fill both tx and tn with nan\n",
    "# tx=xr.where(tx<tn,np.nan,tx)\n",
    "# tn=xr.where(tx<tn,np.nan,tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # is tx ever less than tn now?\n",
    "# (tn>tx).data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan for daily temperature greater than 70C (158F) or less than -70C (-94F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is tx>70C, tx<-70C, tn>70C, or tn<-70C?\n",
    "((tx>70)|(tx<-70)).data.sum(), ((tn>70)|(tn<-70)).data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leap days (i.e Feb 29th)\n",
    "\n",
    "here we'll just drop the leap days from the data arrays rather than filling with nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's double check that the time dimension is the same for \n",
    "# all of our data arrays\n",
    "assert list(pr.time.data)==list(tx.time.data), 'pr.time and tx.time are not equal'\n",
    "assert list(pr.time.data)==list(tn.time.data), 'pr.time and tn.time are not equal'\n",
    "\n",
    "# another way to do the same thing without having to convert data structure is with numpy.testing\n",
    "npt.assert_array_equal(pr.time,tx.time,'pr.time and tx.time are not equal')\n",
    "npt.assert_array_equal(pr.time,tn.time,'pr.time and tn.time are not equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find all the leap days\n",
    "# leapdays=pr.time[(pr.time.dt.day==29) & (pr.time.dt.month==2)]\n",
    "# leapdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indexes to all the leap days\n",
    "leap_ind=np.where((pr.time.dt.day==29) & (pr.time.dt.month==2))[0]\n",
    "leap_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop the leap days from the data arrays\n",
    "# pr=pr.drop_sel(time=leapdays)\n",
    "# tx=tx.drop_sel(time=leapdays)\n",
    "# tn=tn.drop_sel(time=leapdays)\n",
    "# len(pr)\n",
    "\n",
    "# fill with nan\n",
    "# pr=pr.where((pr.time.dt.day==29) & (pr.time.dt.month==2),np.nan,pr)\n",
    "pr[leap_ind]=np.nan\n",
    "tx[leap_ind]=np.nan\n",
    "tn[leap_ind]=np.nan\n",
    "pr[leap_ind]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan for impossible dates (i.e. 32nd March, 12th June 2042)\n",
    "\n",
    "This data has datetimes for the time dimension. If there were impossible dates, xarray would have had a problem at the open_dataarray statement. So we know there are no impossible dates present.\n",
    "\n",
    "There could be dates missing, but we can check that just by looking at the length of the time dimension. We have 45 years of daily data, now without leap days. 45years * 365days = 16425days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.shape, tx.shape, tn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nan for non-numeric values\n",
    "\n",
    "Similarly, use of netcdf and xarray ensures that there are no non-numeric values. Each variable in the data file is of one data type (e.g. float) and if there were a non-float value present there would have been an error already. We can be assured the data we've read is all float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.dtype, tx.dtype, tn.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### daily temperature outliers (i.e. 3-5 times the standard deviation from the mean value for each calendar day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the time-mean for each day of the year\n",
    "tx_daily_mean=tx.groupby(tx.time.dt.dayofyear).mean('time')\n",
    "tn_daily_mean=tn.groupby(tn.time.dt.dayofyear).mean('time')\n",
    "tx_daily_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the standard deviation for each day of the yar\n",
    "# .std throws a runtime warning about degrees of freedom because of \n",
    "# nan in the data so we supress the warnings here\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
    "    tx_stddev=tx.groupby(tx.time.dt.dayofyear).std('time')\n",
    "    tn_stddev=tn.groupby(tn.time.dt.dayofyear).std('time')\n",
    "tx_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define daily outlier temperature as exceeding the mean +/- 3 times standard deviation\n",
    "tx_outlier_upper, tx_outlier_lower=(tx_daily_mean+tx_stddev*5), (tx_daily_mean-tx_stddev*5)\n",
    "tn_outlier_upper, tn_outlier_lower=(tn_daily_mean+tn_stddev*5), (tn_daily_mean-tn_stddev*5)\n",
    "tx_outlier_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tx',(tx.groupby(tx.time.dt.dayofyear)>tx_outlier_upper).data.sum(), (tx.groupby(tx.time.dt.dayofyear)<tx_outlier_lower).data.sum())\n",
    "print('tn',(tn.groupby(tn.time.dt.dayofyear)>tn_outlier_upper).data.sum(), (tn.groupby(tn.time.dt.dayofyear)<tn_outlier_lower).data.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's also look at how many missing values we have per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that sums the number of nans in each month of data\n",
    "def get_nans_per_month(data_in):\n",
    "    month_groups=pd.MultiIndex.from_arrays([data_in.time['time.year'].data,data_in.time['time.month'].data])\n",
    "    data_in.coords['month_groups']=('time',month_groups)    \n",
    "    nancount=data_in.isnull().groupby('month_groups').sum()\n",
    "    return nancount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_nan_per_month=get_nans_per_month(pr.copy())\n",
    "tx_nan_per_month=get_nans_per_month(tx.copy())\n",
    "tn_nan_per_month=get_nans_per_month(tn.copy())\n",
    "# pr_nan_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetimes for the x axis\n",
    "time_months=pd.date_range(tx.time.data[0],tx.time.data[-1],freq='MS')\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "ax=fig.add_subplot(311)\n",
    "plt.plot(time_months,pr_nan_per_month)\n",
    "plt.title('prcp, number of nans per month')\n",
    "\n",
    "ax=fig.add_subplot(312)\n",
    "plt.plot(time_months,tx_nan_per_month)\n",
    "plt.title('tmax, number of nans per month')\n",
    "\n",
    "ax=fig.add_subplot(313)\n",
    "plt.plot(time_months,tn_nan_per_month)\n",
    "plt.title('tmin, number of nans per month')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate climate change indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Maximum Value of Daily Minimum Temperature (TNx)\n",
    "\n",
    "- max(each month of daily minimum temperature values)\n",
    "\n",
    "Here we are inputting daily data and pulling out 1 value per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is similar to how we found nans per month\n",
    "\n",
    "# create an index for every month in the timeseries\n",
    "month_groups=pd.MultiIndex.from_arrays([tn.time['time.year'].data,tn.time['time.month'].data])\n",
    "\n",
    "# add the month_groups index as a new coordinate (labels)\n",
    "# the month_groups coordinate will allow us to groupby months in the next step \n",
    "# each day of data in any particular month will be associated with a label in the month_groups coordinate\n",
    "tn.coords['month_groups']=('time',month_groups)    \n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now groupby month and find the maximum value of each month\n",
    "# this should return 1 value for every month, 45 years x 12 months = length 540\n",
    "TNx=tn.groupby('month_groups').max()\n",
    "TNx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the monthly datetimes that we created earlier (time_months) for the x axis values, plot TNx\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_months,TNx) # plt.plot is from matplotlib.pyplot\n",
    "plt.title('Monthly Maximum Value of Daily Minimum Temperature (TNx)')\n",
    "plt.ylabel('degrees C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual Total Precip Amount Over 99th Percentile on Wet Days (R99pTOT)\n",
    "\n",
    "- annually, the sum of precipitation when precipitation is > 99th percentile of wet day precipitation in the base period 1981-2010\n",
    "- where a wet day is precipitation >= 1mm\n",
    "\n",
    "Here we first use daily data during the base period to determine the 99th percentile of wet day precipitation. Then for each year of daily data we determine if each day exceeds the threshold and calculate an annual sum of precip on days that exceed the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the leap days by label\n",
    "leapdays=pr.time[(pr.time.dt.day==29) & (pr.time.dt.month==2)]\n",
    "leapdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop leapdays from the data\n",
    "pr_noleap=pr.drop_sel(time=leapdays)\n",
    "\n",
    "len(pr_noleap),len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice to only base years\n",
    "pr_baseyrs=pr_noleap.sel(time=slice('1981','2010'))\n",
    "pr_baseyrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 99th percentile of wet day precipitation\n",
    "pr_99w=xr.where(pr_baseyrs>=1.0,pr_baseyrs,np.nan).quantile(0.90)\n",
    "pr_99w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with nan where pr doesn't meet the criteria\n",
    "pr_noleap_copy=pr_noleap.copy()\n",
    "pr_noleap_copy=xr.where(pr_noleap_copy>pr_99w,pr_noleap_copy,np.nan)\n",
    "\n",
    "# then sum over each year\n",
    "R99pTOT=pr_noleap_copy.groupby(pr_noleap_copy.time.dt.year).sum()\n",
    "R99pTOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(R99pTOT.year,R99pTOT)\n",
    "plt.title('Annual Total Precip Amount Over 99th Percentile on Wet Days (R99pTOT)')\n",
    "plt.ylabel('mm/year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Maximum Consecutive 5-day Precipitation (Rx5day)\n",
    "\n",
    "- max(5-day rolling mean precipitation within each month)\n",
    "\n",
    "Here we are inputting daily data, for each month calculating the mean precipitation amount for each 5-day window of data values, then choosing the maximum of 5-day window value for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll again use the pr timeseries without leapdays\n",
    "# we want to group by each month\n",
    "# we'll assign an index as a new coordinate for this \n",
    "month_groups_noleap=pd.MultiIndex.from_arrays([pr_noleap.time['time.year'].data,pr_noleap.time['time.month'].data])\n",
    "pr_noleap.coords['month_groups_noleap']=('time',month_groups_noleap) \n",
    "pr_noleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at how to use .groupby\n",
    "# see what happens if we group by month_groups\n",
    "# there should be 45 years x 12 months = 540 groups of data\n",
    "# .groupby returns an object that can be iterated over in the form (label,group_array_of_data) pairs\n",
    "pr_noleap.groupby(pr_noleap.month_groups_noleap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to access the group labels\n",
    "pr_noleap.groupby(pr_noleap.month_groups_noleap).groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to access the array indexes assigned to a group label\n",
    "label=(1979,5)\n",
    "pr_noleap.groupby(pr_noleap.month_groups_noleap).groups[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to access the array data assigned to a group label\n",
    "pr_noleap.groupby(pr_noleap.month_groups_noleap)[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to find the maximum value of the 5-day rolling mean in each month\n",
    "# let's test with one month first\n",
    "window_len = 5  # days\n",
    "\n",
    "# using rolling to divide the month of data up into 5-day windows\n",
    "# then take the mean of each 5-day window if all 5 days in the window have finite data values (not nan)\n",
    "# then find the maximum value of the above\n",
    "# the result should be a single value\n",
    "pr_noleap.groupby(pr_noleap.month_groups_noleap)[label].rolling(time=window_len,min_periods=window_len).mean().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the calculation for all months using a for loop\n",
    "results=[]  # empty list\n",
    "\n",
    "# loop through each label,data pair in the groupby object\n",
    "for label,data_group in pr_noleap.groupby(pr_noleap.month_groups_noleap):\n",
    "    # append the result for each month to our results list\n",
    "    results.append(data_group.rolling(time=window_len,min_periods=window_len).mean().max())\n",
    "\n",
    "# the result should be 1 value for every month (45 years x 12 months = length 540)\n",
    "print(len(results))\n",
    "\n",
    "# look at the first item in the list\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though each monthly calculation returns a single value\n",
    "# that value is returned as an xarray DataArray with some metadata attached\n",
    "# to get all our results back into a single object we can concatenate the list of xr.DataArrays\n",
    "\n",
    "Rx5day=xr.concat(results,dim='time') # concat on a new dimension called time\n",
    "Rx5day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the new time dimension a coordinate (supply time labels)\n",
    "\n",
    "# first make an array of datetimes\n",
    "print(pr_noleap.time.data[0],pr_noleap.time.data[-1]) # first and last time in string format\n",
    "\n",
    "time_months=pd.date_range(pr_noleap.time.data[0],pr_noleap.time.data[-1],freq='MS') # full array of datetimes\n",
    "\n",
    "print(time_months[0:4])\n",
    "\n",
    "# assign the time coordinate labels\n",
    "Rx5day.coords['time']=('time',time_months)    \n",
    "\n",
    "# while we're at it, assign some variable attributes\n",
    "# if we use xarray plotting, some of this will show up automagically on the plot\n",
    "var_atts={'standard_name':'precipitation','units':'mm/day','description':'monthly maximum of monthly 5-day rolling mean prcp'}\n",
    "Rx5day.attrs=var_atts\n",
    "\n",
    "Rx5day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# use xarray plotting (which is based on matplotlib) by calling .plot on the xr.DataArray object \n",
    "fig=plt.figure(figsize=(15,2))\n",
    "Rx5day.plot() \n",
    "plt.title('Monthly Maximum Consecutive 5-day Precipitation (Rx5day)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Length of Consecutive Dry Days (CDD)\n",
    "\n",
    "- annually, the maximum length of consecutive days where precipitation is < 1mm\n",
    "- not looking at dry spells that span over multiple years, just cutting off the search at the end of each year\n",
    "\n",
    "Here we are inputting daily data, determining whether each day falls under the precipitation threshold, and finding the longest period of consecutive days each year that meets the threshold requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use pr_noleap again \n",
    "pr_noleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mask where 1=dry and 0=wet\n",
    "threshold = 1  # mm/day\n",
    "data_mask = xr.where(pr_noleap<threshold,1,0)\n",
    "data_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by year\n",
    "data_mask_grouped=data_mask.groupby(data_mask.time.dt.year)\n",
    "data_mask_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nested for loops to find the longest stretch of dry days in each year\n",
    "\n",
    "CDD=[] # empty list to store results\n",
    "\n",
    "# loop through each year\n",
    "for year,data1yr in data_mask_grouped:\n",
    "    \n",
    "    counter=0\n",
    "    longest_counter=0\n",
    "\n",
    "    # loop through each day\n",
    "    for iday in range(len(data1yr)):\n",
    "\n",
    "        # if it's a dry day, increment the counter\n",
    "        if data1yr[iday]==1: counter+=1\n",
    "\n",
    "        # keep track of the longest consecutive amount of dry days\n",
    "        if counter>longest_counter: longest_counter=counter\n",
    "\n",
    "        # if it's not a dry day (0 or nan), start the counter over at 0\n",
    "        if data1yr[iday]!=1: counter=0\n",
    "\n",
    "    # add to the dictionary\n",
    "    CDD.append(longest_counter)\n",
    "    longest_counter=0\n",
    "    counter=0\n",
    "\n",
    "# look at the results for first 5 years\n",
    "CDD[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual datetimes for x axis values\n",
    "time_annual=pd.date_range(tx.time.data[0],tx.time.data[-1],freq='YS')\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_annual,CDD)\n",
    "plt.title('Maximum Length of Consecutive Dry Days (CDD)')\n",
    "plt.ylabel('days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing Season Length (GSL)\n",
    "\n",
    "- annually, growing season starts on the first day of the first six consecutive day period where daily mean temperature is > 5C\n",
    "- annually, growing season ends on the first day after 1 July of the first six consecutive day period where daily mean temperature is < 5C\n",
    "\n",
    "Here we are inputting daily data, pulling out 2 dates per year, and calculating the number of days between the two dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5   # degrees C\n",
    "window_len = 6  # consectutive days\n",
    "\n",
    "# calculate mean temperature\n",
    "t_mean=(tn+tx)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the start and end of the growing season\n",
    "# we will need to \"roll\" through time.\n",
    "# in order for the leap days (that we filled with nan)\n",
    "# to not mess us up, we'll need to drop those days from the data\n",
    "\n",
    "# leapdays=t_mean.time[(t_mean.time.dt.day==29) & (t_mean.time.dt.month==2)]\n",
    "t_mean_noleap=t_mean.drop_sel(time=leapdays)\n",
    "len(leapdays),len(t_mean_noleap),len(t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll want to group by years to find the start and end of the growing season for each year\n",
    "# in this case we don't need to assign a new coordinate to use groupby\n",
    "# we can use .dt on xr.DataArrays of datetimes to select/subset/group (xarray .dt operates the same as pandas .dt)\n",
    "t_mean_noleap.groupby(t_mean_noleap.time.dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to access the group labels\n",
    "# t_mean_noleap.groupby(t_mean_noleap.time.dt.year).groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to access the array indexes assigned to a group label\n",
    "# testyear=1979\n",
    "# t_mean_noleap.groupby(t_mean_noleap.time.dt.year).groups[testyear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to access the array data assigned to a group label\n",
    "# data_1yr=t_mean_noleap[t_mean_noleap.groupby(t_mean_noleap.time.dt.year).groups[testyear]]\n",
    "year=1979\n",
    "data_1yr=t_mean_noleap.groupby(t_mean_noleap.time.dt.year)[year]\n",
    "data_1yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mask for where daily temperature is greater than 5C\n",
    "data_mask=xr.where(data_1yr>threshold,1,0)\n",
    "data_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the timeseries into windows of length 6\n",
    "# the first window has the first value of the data_mask (index 0) in the last position of the window \n",
    "# plus the 5 preceeding values of the data_mask, which in this case are nan because we're at the beginning of the data_mask array\n",
    "# the 6th window (index 5) should be equal to the first 6 values of data_mask (no nans)\n",
    "\n",
    "# create the windows\n",
    "windows=data_mask.rolling(time=window_len,center=False).construct('window')\n",
    "\n",
    "# print the window with the first 6 values of data_mask\n",
    "print(windows.isel(time=window_len-1))\n",
    "\n",
    "# print array info\n",
    "windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the sum of each window\n",
    "# this will tell us how many days per window are over the 5C threshold\n",
    "# ignore windows that contain any nans\n",
    "windows.sum('window',min_count=window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indexes of each 6-day window where all days were over the 5C threshold\n",
    "# np.where returns a tuple in this case where the resulting array is \n",
    "# in the first index of the tuple. this is why we use the [0] to pull the array from the tuple\n",
    "np.where(windows.sum('window')==window_len)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take the first value of the result above\n",
    "# this is the first window where the requirement was met (6 days above 5C)\n",
    "# the windows are indexed as their last day, i.e. the 5th index window contains index days 0,1,2,3,4,5 \n",
    "# the start of the growing season is the first day of the first 6-day period meeting the 5C requirement\n",
    "# so to get the index of the first day in the window we subtract 5\n",
    "gs_start_ind=np.where(windows.sum('window')==window_len)[0][0] - (window_len-1)\n",
    "gs_start_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's search for the end of the growing season\n",
    "# we know to only look after July 1, what index is that?\n",
    "label=str(year)+'-07-01'\n",
    "minval=t_mean_noleap.indexes['time'].get_loc(label)\n",
    "minval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do similar steps to find the end of the growing season\n",
    "\n",
    "# 0/1 mask for where temperature is less than 5C\n",
    "data_mask=xr.where(data_1yr<threshold,1,0)\n",
    "data_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up into 6-day windows\n",
    "windows=data_mask.rolling(time=window_len,center=False).construct('window')\n",
    "windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index to all the windows where temperature is always less than 5C\n",
    "np.where(windows.sum('window')==window_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index to all the windows where temperature is lt 5C\n",
    "# but remember the index is to the last date in the window and we want the index of the first date\n",
    "# so we subtract 5 and save the result as an array\n",
    "possible_inds=np.where(windows.sum('window')==window_len)[0] - (window_len-1)\n",
    "possible_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset possible_inds to only indexes that correspond to days after July 1\n",
    "# and take the first value in that result\n",
    "gs_end_ind=possible_inds[possible_inds>minval][0]\n",
    "gs_end_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to do what we did above for each year\n",
    "# we'll put these functions in a loop below\n",
    "\n",
    "def get_gs_start(data_1yr):\n",
    "    mask=xr.where(data_1yr>threshold,1,0)\n",
    "    windows=mask.rolling(time=window_len,center=False).construct('window')\n",
    "    ind=np.where(windows.sum('window')==window_len)[0][0] - (window_len-1)\n",
    "    return ind\n",
    "\n",
    "def get_gs_end(data_1yr):\n",
    "    mask=xr.where(data_1yr<threshold,1,0)\n",
    "    windows=mask.rolling(time=window_len,center=False).construct('window')\n",
    "    # sometimes it may be warm through the end of the year\n",
    "    # in these cases we would end up with an error if no windows meet the <5C requirement\n",
    "    # try/except works to pass in the last day of the year as the end of the growing season in these cases\n",
    "    try:\n",
    "        possible_inds=np.where(windows.sum('window')==window_len)[0]\n",
    "        ind=possible_inds[possible_inds>minval][0]\n",
    "    except:\n",
    "        ind=364 # index of the last day of year\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by year and loop through each year's worth of data\n",
    "# to find the index of the start and end of the growing season \n",
    "\n",
    "# create empty lists for storing results\n",
    "gs_start_list=[]\n",
    "gs_end_list=[]\n",
    "\n",
    "# loop through years of data\n",
    "# .groupby returns an object that can be iterated over in the form (label,group_array_of_data) pairs\n",
    "# here \"label\" are the years\n",
    "# and \"group\" are a year's worth of data values\n",
    "for label,group in t_mean_noleap.groupby(t_mean_noleap.time.dt.year):\n",
    "    # call our functions and append the result to our lists\n",
    "    gs_start_list.append(get_gs_start(group))\n",
    "    gs_end_list.append(get_gs_end(group))\n",
    "\n",
    "# look at the first 5 values of each list\n",
    "gs_start_list[0:5], gs_end_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check our work\n",
    "assert all(x>=0 for x in gs_start_list), \"negative values in gs_start_list\"\n",
    "assert all(x<=364 for x in gs_start_list), \"values>364 in gs_start_list\"\n",
    "assert all(x>=minval for x in gs_end_list), f\"values<{minval} in gs_end_list\"\n",
    "assert all(x<=364 for x in gs_start_list), \"values>364 in gs_end_list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the growing season length for each year\n",
    "# this is called a \"list comprehension\"\n",
    "# it executes a for loop and returns results inside a list\n",
    "GSL = [end_ind-start_ind for end_ind,start_ind in zip(gs_end_list,gs_start_list)] \n",
    "\n",
    "# # below is identical to above\n",
    "# GSL=[]\n",
    "# for end_ind,start_ind in zip(gs_end_list,gs_start_list):\n",
    "#     GSL.append(end_ind-start_ind)\n",
    "\n",
    "# check out the first 5 values\n",
    "GSL[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using annual datetimes that we've already created for x axis values\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_annual,GSL)\n",
    "plt.title('Growing Season Length (GSL)')\n",
    "\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_annual,gs_start_list)\n",
    "plt.title('Start of Growing Season DOY')\n",
    "\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_annual,gs_end_list)\n",
    "plt.title('End of Growing Season DOY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Spell Duration Index (WSDI)\n",
    "\n",
    "- 6 consecutive days of hot maximum temperatures\n",
    "- hot temperature threshold defined as > 90th percentile of maximum temperature for each calendar day using a centered 5-day window in the base period 1981-2010\n",
    "- warm spells that contain dates for multiple years are assigned to the year when the spell ends\n",
    "\n",
    "Here we first use daily data during the base period to determine the daily 90th percentile temperature threshold. Then using all years of daily data we decide whether each calendar day exceeds the hot threshold, then find occurrences where the threshold is exceeded for at least 6 consecutive days (this is a warm spell), then sum the number of days annually in the warm spells.\n",
    "\n",
    "Notice that this is not the same as finding dangerous heat waves with respect to human health because it is based on a temperature threshold for each calendar day. This means that the WSDI will include winter warm spells where the temperature exceeds the 90th percentile of winter daily temperature, which would likely be a comfortable temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's find the 90th percentile temperature for each calendar day (using a centered 5 day window)\n",
    "# this means that to determine the 90th percentile temperature for a given day we need \n",
    "# that day's temperature in each year as well as the temperature for 2 days before and 2 days after in each year\n",
    "# we'll set it up to find the answer for 1 day first and then make a loop to compute all other days\n",
    "\n",
    "# starting with Feb 1\n",
    "\n",
    "\n",
    "\n",
    "n_baseyrs=30\n",
    "# base_first='1981'\n",
    "# base_last='2010'\n",
    "\n",
    "day_first=1\n",
    "day_last=365\n",
    "doy=list(np.arange(day_first,day_last+1))*n_baseyrs\n",
    "doy= [364,365] + doy + [1,2]\n",
    "\n",
    "len(doy), doy[0:9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_noleap_baseyrs=tx.drop_sel(time=leapdays).sel(time=slice('1980-12-30','2011-01-02'))\n",
    "tx_noleap_baseyrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_noleap_baseyrs.coords['doy_noleap']=('time',doy)\n",
    "tx_noleap_baseyrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_windows=tx_noleap_baseyrs.rolling(time=5,center=True).construct('window')\n",
    "tx_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the windows centered on the extra dates\n",
    "tx_windows=tx_windows.drop_sel(time=['1980-12-30','1980-12-31','2011-01-1','2011-01-02'])\n",
    "tx_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now groupby our doy index 'doy_noleap'\n",
    "# each group will contain the temperature for a single doy of every year plus the two days before and two days after\n",
    "# in other words, each group is the 5-day centered window for a given doy for all years \n",
    "# 5 days * 30 years = 150 data values in each group\n",
    "tx_grouped=tx_windows.groupby(tx_windows.doy_noleap)\n",
    "\n",
    "# let's look at what is in a data group for doy 15\n",
    "tx_grouped[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now find the 90th percentile of data values in each group\n",
    "# we should end up with 1 value for each doy of the year (excluding leap days)\n",
    "threshold90=tx_grouped.quantile(0.9,dim=['time','window'])\n",
    "threshold90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep tx for comparison to threshold90\n",
    "# this time use all data (don't subset in time)\n",
    "nyears=45\n",
    "doy=list(np.arange(day_first,day_last+1))*nyears\n",
    "tx_noleap=tx.drop_sel(time=leapdays)\n",
    "tx_noleap.coords['doy_noleap']=('time',doy)\n",
    "tx_noleap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which days exceed threshold90\n",
    "# these are the hot days\n",
    "tx_hot_mask = tx_noleap.groupby(tx_noleap.doy_noleap) > threshold90\n",
    "tx_hot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many True days and how many False?\n",
    "ntrue=tx_hot_mask.sum()\n",
    "nfalse=len(tx_hot_mask)-ntrue\n",
    "ntrue,nfalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_hot_mask.time.isel(time=6).data + np.timedelta64(1,'D')#.data#,tx_hot_mask.time.isel(time=6).data.timedelta(days=1)\n",
    "# tx_hot_mask.time.isel(time=6).dt.strftime(\"%a, %b %d %H:%M\").data\n",
    "# tx_hot_mask.time.isel(time=6).dt.strftime(\"%Y-%m-%d\").timedelta(days=1)\n",
    "window_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=6\n",
    "np.arange(i-(window_len-1),i+1)\n",
    "\n",
    "# tx_hot_mask.isel(time=6)\n",
    "date=tx_hot_mask.time.isel(time=i).data\n",
    "print(date)\n",
    "dates=pd.date_range(date-np.timedelta64(window_len-1,'D'),date-np.timedelta64(0,'D'))\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll loop in time to identify warm spells in each year and sum the days in warm\n",
    "count=0\n",
    "hot_inds=[]\n",
    "hot_dates=[]\n",
    "# event_year=[]\n",
    "\n",
    "for i,value in enumerate(tx_hot_mask):\n",
    "    if value: count=count+1 # if True start a counter\n",
    "    else: count=0\n",
    "\n",
    "    if count>=window_len:\n",
    "        inds=np.arange(i-(window_len-1),i+1)\n",
    "        hot_inds.extend(inds)\n",
    "\n",
    "        # date=tx_hot_mask.time.isel(time=i).data\n",
    "        # dates=pd.date_range(date-np.timedelta64(window_len-1,'D'),date-np.timedelta64(0,'D'))\n",
    "        # hot_dates.extend(dates)\n",
    "     \n",
    "\n",
    "    # if i < window_len-1:\n",
    "len(hot_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_inds=np.unique(hot_inds)\n",
    "len(hot_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_dates=np.unique(hot_dates)\n",
    "len(hot_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to know which year each warm spell event takes place in\n",
    "# warm spell days are counted for the year when the spell ends\n",
    "# are there any warm spells that span over two years?\n",
    "ind1=None\n",
    "event_inds=[]\n",
    "for i,value in enumerate(hot_inds[:-1]):\n",
    "    if ind1==None:\n",
    "        ind1=value\n",
    "\n",
    "    if hot_inds[i+1]==value+1:\n",
    "        pass\n",
    "    else:\n",
    "        ind2=value\n",
    "        event_inds.append((ind1,ind2)) # append a tuple\n",
    "        ind1=hot_inds[i+1]\n",
    "\n",
    "print(len(event_inds))\n",
    "event_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_hot_mask.isel(time=slice(544,566))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_hot_mask.time.isel(time=event_inds[0][0]).dt.year.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do owe have any warm spells that span over multiple years\n",
    "spell_year=[]\n",
    "count=0\n",
    "for startstop in event_inds:\n",
    "    year_start=tx_hot_mask.time.isel(time=startstop[0]).dt.year.data\n",
    "    year_end = tx_hot_mask.time.isel(time=startstop[1]).dt.year.data\n",
    "    if year_start==year_end:\n",
    "        spell_year.append(year_start)\n",
    "    else:\n",
    "        spell_year.append(year_end)\n",
    "        count+=1\n",
    "print(count)\n",
    "spell_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count days in warm spells per year\n",
    "WSDI=[]\n",
    "for data_year in np.arange(1979,2023+1):\n",
    "    day_count=0\n",
    "    for i,event_year in enumerate(spell_year):\n",
    "        if event_year==data_year:\n",
    "            ndays=event_inds[i][1]-event_inds[i][0]+1\n",
    "        else:\n",
    "            ndays=0    \n",
    "        day_count=day_count+ndays\n",
    "    WSDI.append(day_count)\n",
    "len(WSDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using annual datetimes that we've already created for x axis values\n",
    "# plot\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_annual,WSDI)\n",
    "plt.title('Warm Spell Duration Index (WSDI)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting linear trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to clean up our variable's coordinate labels\n",
    "# this is because the polyfit function we will use doesn't like coordinates like our multi-index \"month_groups\"\n",
    "\n",
    "TNx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNx=TNx.drop_vars(['month_groups', 'time_level_0', 'time_level_1']) # delete the junk\n",
    "TNx=TNx.rename({'month_groups':'time'})  # rename the dimension\n",
    "TNx=TNx.assign_coords({'time':time_months})  # assign new coordinate labels to time dim\n",
    "\n",
    "# nice and clean\n",
    "TNx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate least squares linear regression coefficients\n",
    "coefs=TNx.polyfit(dim='time',deg=2)\n",
    "\n",
    "# generate the x,y points of the linear regression line\n",
    "regline=xr.polyval(TNx.time,coefs)\n",
    "\n",
    "# see our handywork\n",
    "regline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the linear regression over the timeseries TNx\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_months,TNx) # plt.plot is from matplotlib.pyplot\n",
    "regline.polyfit_coefficients.plot(linestyle='--')\n",
    "plt.title('Monthly Warmest Night (TNx) with linear trend')\n",
    "plt.ylabel('degrees C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's plot the seasonal mean TNx with the linear trend for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNx_seasonal=TNx.resample(time='QS-DEC').mean()\n",
    "TNx_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNx_DJF=TNx_seasonal[0::4][1:-1]\n",
    "TNx_MAM=TNx_seasonal[1::4]\n",
    "TNx_JJA=TNx_seasonal[2::4]\n",
    "TNx_SON=TNx_seasonal[3::4]\n",
    "\n",
    "coefs=TNx_DJF.polyfit(dim='time',deg=2)\n",
    "reg_DJF=xr.polyval(TNx_DJF.time,coefs)\n",
    "\n",
    "coefs=TNx_MAM.polyfit(dim='time',deg=2)\n",
    "reg_MAM=xr.polyval(TNx_MAM.time,coefs)\n",
    "\n",
    "coefs=TNx_JJA.polyfit(dim='time',deg=2)\n",
    "reg_JJA=xr.polyval(TNx_JJA.time,coefs)\n",
    "\n",
    "coefs=TNx_SON.polyfit(dim='time',deg=2)\n",
    "reg_SON=xr.polyval(TNx_SON.time,coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the linear regression over the timeseries TNx\n",
    "fig=plt.figure(figsize=(15,6))\n",
    "fig.add_subplot(411)\n",
    "plt.plot(TNx_DJF.time,TNx_DJF)\n",
    "plt.ylim(TNx_seasonal.min(),TNx_seasonal.max())\n",
    "reg_DJF.polyfit_coefficients.plot(linestyle='--')\n",
    "plt.title('Winter Mean Warmest Night')\n",
    "plt.ylabel('degrees C')\n",
    "\n",
    "fig.add_subplot(412)\n",
    "plt.plot(TNx_MAM.time,TNx_MAM)\n",
    "plt.ylim(TNx_seasonal.min(),TNx_seasonal.max())\n",
    "reg_MAM.polyfit_coefficients.plot(linestyle='--')\n",
    "plt.title('Spring Mean Warmest Night')\n",
    "plt.ylabel('degrees C')\n",
    "\n",
    "fig.add_subplot(413)\n",
    "plt.plot(TNx_JJA.time,TNx_JJA)\n",
    "plt.ylim(TNx_seasonal.min(),TNx_seasonal.max())\n",
    "reg_JJA.polyfit_coefficients.plot(linestyle='--')\n",
    "plt.title('Summer Mean Warmest Night')\n",
    "plt.ylabel('degrees C')\n",
    "\n",
    "fig.add_subplot(414)\n",
    "plt.plot(TNx_SON.time,TNx_SON)\n",
    "plt.ylim(TNx_seasonal.min(),TNx_seasonal.max())\n",
    "reg_SON.polyfit_coefficients.plot(linestyle='--')\n",
    "plt.title('Fall Mean Warmest Night')\n",
    "plt.ylabel('degrees C')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the Changes In Value of These Indices Over Time Statistically Significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if trend is statistically different from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Climate Change Indices on Gridded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download/unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat one of the above analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!\n",
    "\n",
    "### Choose one of three coding mini-projects below to complete on your own and prepare to share your findings\n",
    "\n",
    "\n",
    "**Option 1 (easiest):** Calculate the monthly mean daily temperature range (DTR) and create a figure showing the DTR timeseries. \n",
    "\n",
    "&emsp;Hints:\n",
    "- Use daily tmax and tmin data\n",
    "- Calculate the daily temperature range as tmax-tmin\n",
    "- For each month, find the mean of the range values you calculated in the previous step\n",
    "- Plot your timeseries of monthly values. Include axis labels and a title. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Option 2 (moderate):** Calculate the cold spell duration index (CDSI) at the xx station and create a figure showing the CDSI timeseries. Extra: see if you can determine whether the change in the CDSI is statistically significant.\n",
    "\n",
    "&emsp;Hints:\n",
    "- Use daily tmin data\n",
    "- Find the daily 10th percentile temperature using a centered 5-day window over the base period 1961-1990\n",
    "- Using all data years, determine if each day exceeds the threshold (looking for days with tmin < threshold)\n",
    "- Identify cold spells as periods of 6 consecutive days when the temperature exceeds the threshold\n",
    "- Count how many total cold spell days there are annually (remember each cold spell is assigned to the year when the spell ends)\n",
    "- Plot the timeseries of annual values. Include axis labels and a title.\n",
    "- Extra Step: Determine statistical significance of the trend line (linear regression) or the difference in means between two 30-year periods (1941-1970) and (1991-2020).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**Option 3 (hardest):** Use a gridded dataset to compute the annual growing season length (GSL) at each grid cell. Then, calculate the trend in GSL at each grid cell and also determine whether each trend is statisically significant. Present your results in a figure that shows the GSL trend for each grid cell (on a map) and include an indication of whether each grid cell value is statistically significant.\n",
    "\n",
    "&emsp;Hints:\n",
    "- Use gridded daily tmax and tmin data\n",
    "- Calculate daily mean temperature\n",
    "- Use the same process we showed previously to determine the annual start/end of the growing season and find the annual GSL, except this time do the calculations at each grid cell.\n",
    "- Calculate the trend (linear regression) in annual GSL at each grid cell.\n",
    "- Determine if each trend is statistically significant.\n",
    "- Plot the the map of trend values and indicate significance at each grid with hatching or some other visual indicator. Include a title and legend.\n",
    "\n",
    "\n",
    "# might need to replace one of the above with\n",
    " Option: reproduce any of the indices in this notebook using gridded data for Mississippi\n",
    "\n",
    " Option: use dask to speed up computation on gridded data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at the answer figure for option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at the answer figure for option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at the answer figure for option 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to create answer codes for these and put them in the repo. Direct learners to answers after the work-on-your-own session."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 ('pyworkshop8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ab2d50a2133bd9963a4e0ed82a6ab96066a8de036f78eb77258e6d5c22f3d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
