{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159d059b-7cfa-4106-a540-73775cd20311",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: Climate Change Indicators, Your Turn! Option 3 Answer Key\n",
    "\n",
    "**Developed By:** Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "**Original Development Date:** May 2024\n",
    "\n",
    "**Package Requirements:** xarray, netcdf4, numpy, pandas, scipy, matplotlib, jupyter\n",
    "\n",
    "**Links:** **[OSF project link](https://osf.io/zhpd5/)**, [link to this notebook on github](https://github.com/kerriegeil/MSU_py_training/blob/main/learn_by_doing/climate_change_indicators/assignments/climate_change_indicators_option3.ipynb)\n",
    "\n",
    "---\n",
    "**Assignment:**\n",
    "\n",
    "Calculate the warm spell duration index (WSDI) at Starkville, determine whether there is a statistically significant trend, and create a figure showing the WSDI timeseries, linear trend, and p value. Compare your results to climdex.org, looking at the trend in WSDI over CONUS for similar data years (1977-2021) and also for data years 1922-2021 from the Berkeley Earth Surface Temperature dataset.\n",
    "\n",
    "&emsp;Hints:\n",
    "- Use daily tmax data but drop leap days entirely\n",
    "- Calculate WSDI using the following criteria\n",
    "    - 6 consecutive days of hot maximum temperatures\n",
    "    - hot temperature threshold defined as > 90th percentile of tx for each calendar day using a centered 5-day window in the base period 1981-2010\n",
    "    - warm spells that contain dates over multiple years are assigned to the year when the spell ends\n",
    "- Determine if there is a statistically significant trend in DTR at the 90% confidence level\n",
    "- Plot the DTR timeseries with its linear trend line. Your plot should have x axis tick labels as years, a y axis label that describes the units, and a title that includes the p value.\n",
    "- Go to **[climdex.org](https://www.climdex.org/access/)** and get the png of WSDI trend over CONUS using data years 1977-2021 from the BEST dataset\n",
    "- Go to **[climdex.org](https://www.climdex.org/access/)** and get the png of WSDI trend over CONUS using data years 1922-2021 from the BEST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5fab7-f1c6-4fb3-b870-6a4049c5d0c6",
   "metadata": {},
   "source": [
    "# Import packages and define workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349f193-e92e-4965-a457-952d8fedbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b130ef6-7ed7-48e5-b9d4-ecff086d4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames \n",
    "tmax_f='data/tmax_AgERA5_Starkville_Daily_1979-2023.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa4a23-e0fa-43d2-8c01-1c38bf2c20d2",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Normally, we would run through the data cleaning suggested by ETCCDI here. However, we've already done the data cleaning steps in climate_change_indicators.ipynb and found that the leap days step was the only step that changed our data arrays. We'll repeat only that step here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02583324-f885-46ab-8ebe-a79a76a058d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = xr.open_dataarray(tmax_f)\n",
    "tx = tx.squeeze()\n",
    "\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edce82c-6cc3-4df8-915c-87eac4337b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### leap days (i.e Feb 29th)\n",
    "\n",
    "# create a boolean array of dim 'time' where leapdays are True and all other days are False\n",
    "isleapday=xr.where((tx.time.dt.day==29) & (tx.time.dt.month==2),True,False)\n",
    "\n",
    "# drop leap days \n",
    "tx=tx.where(~isleapday,drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22b904-768a-439c-8dc1-086a4a2a15ca",
   "metadata": {},
   "source": [
    "#  Climate change indicator: Warm Spell Duration Index (WSDI)\n",
    "\n",
    "- 6 consecutive days of hot maximum temperatures\n",
    "- hot temperature threshold defined as > 90th percentile of tx for each calendar day using a centered 5-day window in the base period 1981-2010\n",
    "- warm spells that contain dates for multiple years are assigned to the year when the spell ends\n",
    "\n",
    "Here we first use daily data during the base period to determine the daily 90th percentile temperature threshold. Then using all years of daily data we decide whether each calendar day exceeds the hot threshold, then find occurrences where the threshold is exceeded for at least 6 consecutive days (this is a warm spell), then sum the number of days annually in the warm spells.\n",
    "\n",
    "Notice that this is not the same as finding dangerous heat waves with respect to human health because it is based on a percentile temperature for each calendar day. This means that the WSDI will include winter warm spells where the temperature exceeds the daily 90th percentile, which would likely be a comfortable temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e24492-168c-4846-b6c6-5333efb11d00",
   "metadata": {},
   "source": [
    "## Step 1: calculate daily 90th percentile temperature\n",
    "\n",
    "- drop leap days\n",
    "- use a centered 5 day window\n",
    "- use a base period 1981-2010 plus two extra days before and after\n",
    "\n",
    "This means that to determine the 90th percentile temperature for a given day we need that day's temperature in each base year as well as the temperature for 2 days before and 2 days after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7dd82-7f6d-40cd-9bc3-ab6f4297c7ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop leap days and subset to the base period but include two extra days before and after\n",
    "# the time labels are real nice for this\n",
    "tx_baseyrs=tx.sel(time=slice('1980-12-30','2011-01-02'))\n",
    "tx_baseyrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ee590-9f70-4892-9b1d-24b16b4da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a day-of-year coordinate label for tx_noleap_baseyrs\n",
    "n_baseyrs=30\n",
    "day_first=1\n",
    "day_last=365\n",
    "\n",
    "# a list of numbers 1 through 365\n",
    "doy=list(np.arange(day_first,day_last+1))\n",
    "\n",
    "# repeat the list to match the number of base years\n",
    "doy=doy*n_baseyrs\n",
    "\n",
    "# add values for the 2 extra days before and after the base years\n",
    "doy= [364,365] + doy + [1,2]\n",
    "\n",
    "# doy should have the same length as tx_noleap_baseyrs.time\n",
    "assert len(doy)==len(tx_baseyrs.time), f'tx_baseyrs time dim has {len(tx_baseyrs.time)} and doy has {len(doy)} values'\n",
    "\n",
    "# look at the first few values of doy\n",
    "doy[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b875e85-d7d0-4866-ad18-2503ca994bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign doy as a new coordinate\n",
    "tx_baseyrs.coords['doy']=('time',doy)\n",
    "tx_baseyrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9b5dc-6df6-4573-9c47-467caab0039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct 5-day centered windows\n",
    "tx_windows=tx_baseyrs.rolling(time=5,center=True).construct('window')\n",
    "tx_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54297f1-b81d-49a7-b952-57af7d313a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the windows centered on days outside the base period\n",
    "tx_windows=tx_windows.drop_sel(time=['1980-12-30','1980-12-31','2011-01-1','2011-01-02'])\n",
    "tx_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c8f26-1274-449b-b1f7-4377de53f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now groupby our doy coordinate\n",
    "# each group will contain the temperature for a single doy of every year plus the two days before and two days after\n",
    "# in other words, each group is the 5-day centered window for a given doy for all years \n",
    "# 5 days * 30 years = 150 data values in each group\n",
    "tx_grouped=tx_windows.groupby(tx_windows.doy)\n",
    "\n",
    "# let's look at what is in a data group for doy 15\n",
    "tx_grouped[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7048609-73bf-4aba-bac5-cd3f221cebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now find the 90th percentile of data values in each group\n",
    "# we should end up with 1 value for each doy of the year (excluding leap days)\n",
    "threshold90=tx_grouped.quantile(0.9,dim=['time','window'])\n",
    "threshold90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9257cb-d7ca-4855-aefc-dbf6f9df0a90",
   "metadata": {},
   "source": [
    "## Step 2: identify every day that is above the daily 90th percentile temperature \n",
    "\n",
    "don't worry about the consecutive days criteria yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a4620-67c8-4c8c-bae7-8357be400672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep tx for comparison to threshold90\n",
    "\n",
    "# and add doy coordinate\n",
    "nyears=45\n",
    "doy=list(np.arange(day_first,day_last+1))*nyears\n",
    "tx.coords['doy']=('time',doy)\n",
    "\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d1ee0-bd63-453e-adba-7b48fa47c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which days exceed threshold90 (these are the hot days)\n",
    "# we'll create a boolean array\n",
    "tx_hot_mask = tx.groupby(tx.doy) > threshold90\n",
    "\n",
    "# how many True days and how many False?\n",
    "ntrue=tx_hot_mask.sum()\n",
    "nfalse=len(tx_hot_mask)-ntrue\n",
    "print(f\"{ntrue.data} hot days and {nfalse.data} days that aren't hot\")\n",
    "\n",
    "tx_hot_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373fe1f-a452-4c80-a5ab-56c4c3655698",
   "metadata": {},
   "source": [
    "## Step 3: identify warm spells and determine which year each warm spell occurs in\n",
    "\n",
    "- warms spells are at least 6 consecutive hot days\n",
    "- warm spells that span over multiple years should be assigned to the year when the spell ends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc4132-b1b0-438c-b14a-8171a9677daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll loop in time to identify warm spells in each year\n",
    "# and save the index of every day in every warm spell\n",
    "ndays=6\n",
    "\n",
    "hot_inds=[]  # empty list to hold the results\n",
    "count=0\n",
    "\n",
    "# loop through the boolean array\n",
    "for i,value in enumerate(tx_hot_mask):\n",
    "\n",
    "    # if True start a counter\n",
    "    if value: count=count+1\n",
    "    else: count=0\n",
    "\n",
    "    # when the count reaches 6\n",
    "    # save all the True indexes to our results list\n",
    "    # the results will include duplicates that we'll remove later\n",
    "    if count>=ndays:\n",
    "        inds=np.arange(i-(ndays-1),i+1)\n",
    "        hot_inds.extend(inds)\n",
    "\n",
    "# now remove duplicates\n",
    "hot_inds=np.unique(hot_inds)\n",
    "\n",
    "# how many total days in all warm spells\n",
    "len(hot_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d26808-8def-4ae3-a4c3-0e0494f8d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start and stop index of each warm spell\n",
    "# so we can assign each warm spell to the correct year\n",
    "\n",
    "events=[]  # empty list to hold the results\n",
    "ind_start=None  # initialize\n",
    "\n",
    "# loop through the index values of all warm spell days\n",
    "for i,value in enumerate(hot_inds[:-1]):\n",
    "    # find the first warm spell start\n",
    "    if ind_start==None:\n",
    "        ind_start=value\n",
    "\n",
    "    # find the ending index and the start of the next warm spell \n",
    "    # if the next index increments by 1 keep searching\n",
    "    if hot_inds[i+1]==value+1:\n",
    "        pass\n",
    "    # if the next index increments by more than 1\n",
    "    # then this is the end and the next value is the start of the next event\n",
    "    else:\n",
    "        ind_end=value\n",
    "        events.append((ind_start,ind_end)) # append a tuple\n",
    "        ind_start=hot_inds[i+1]\n",
    "\n",
    "print(f'there are {len(events)} warm spells in our data')\n",
    "\n",
    "# print the first 5 items in events\n",
    "events[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81244749-784e-4033-b4d8-24f4d77a7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a year to each warm spell and count how many span over more than 1 year\n",
    "\n",
    "spell_year=[]  # empty list to hold the results\n",
    "count=0\n",
    "\n",
    "for ind_start,ind_end in events:\n",
    "    year_start=int(tx_hot_mask.time.isel(time=ind_start).dt.year)#.data\n",
    "    year_end = int(tx_hot_mask.time.isel(time=ind_end).dt.year)#.data\n",
    "    if year_start==year_end:\n",
    "        spell_year.append(year_start)\n",
    "    else:\n",
    "        spell_year.append(year_end)\n",
    "        count+=1\n",
    "\n",
    "print(f'{count} warm spell(s) start in Dec of one year and end in Jan of the next year')\n",
    "\n",
    "# print the first 10 items in spell_year\n",
    "spell_year[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080dad0-db52-45c7-8c70-7b0d50013a34",
   "metadata": {},
   "source": [
    "## Step 4: count how many warm spells days per year and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d034d-d917-472c-a952-e8e9d23ebf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count days in warm spells per year\n",
    "\n",
    "WSDI=[]  # empty list to hold the results\n",
    "\n",
    "# loop through all data years\n",
    "for data_year in np.arange(1979,2023+1):\n",
    "    days_per_year=0  \n",
    "    \n",
    "    # loop through all warm spells\n",
    "    for i,event_year in enumerate(spell_year):\n",
    "        if event_year==data_year:\n",
    "            # get days per event\n",
    "            ndays=events[i][1]-events[i][0]+1\n",
    "            days_per_year=days_per_year+ndays\n",
    "            del ndays\n",
    "        \n",
    "    WSDI.append(days_per_year)\n",
    "\n",
    "# print the first 5 items\n",
    "WSDI[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d4c53-0789-4a87-ae2d-f09a01534e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "\n",
    "# create annual datetimes for x axis values\n",
    "time_annual=pd.date_range(tx.time.data[0],tx.time.data[-1],freq='YS')\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time_annual,WSDI)\n",
    "plt.title('Warm Spell Duration Index (WSDI)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385962c1-edef-47c5-a1e4-5e3b512287e8",
   "metadata": {},
   "source": [
    "# Trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4632b-2691-4e96-b4e5-7734fb8847ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "reg_info=ss.linregress(time_annual.year,WSDI)\n",
    "\n",
    "# convert trend units\n",
    "trend=reg_info.slope*100 # days/year --> days/century\n",
    "\n",
    "# create regression line for plotting\n",
    "regline=reg_info.slope*time_annual.year +reg_info.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67ed66-f85c-4008-993e-7d99f94735b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the linear regression over the data and show p value\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "\n",
    "plt.plot(time_annual.year,WSDI)  # the climate change index\n",
    "plt.plot(time_annual.year,regline,linestyle='--')  # the linear regression line\n",
    "\n",
    "plt.axhline(y=np.asarray(WSDI).mean(),color='grey',linestyle='dashed',linewidth=0.5)  # a guide line, the data mean\n",
    "plt.tick_params(labelright=True, right=True)  # add ticks and tick labels to right-side axis\n",
    "plt.ylabel('days') # y axis label\n",
    "plt.title(f'warm spell duration with trend {trend:.2f} days/century (p={reg_info.pvalue:.3f})')\n",
    "\n",
    "# plt.savefig('figs/cci_option3_figure.png')\n",
    "plt.show() # plot it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993c241-da72-4458-961e-69f793bd9482",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "With a p value of 0.354, there is no statistically significant trend in Starkville WSDI.\n",
    "\n",
    "We find a similar result at Starkville on climdex.org, using BEST data over the years 1977-2021 to calculate WSDI trend. However, there do appear to be significant trends of increasing WSDI across much of the southwest US and Pacific coast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19b94ca4-4c58-4673-9fb0-b6809599d405",
   "metadata": {},
   "source": [
    "<img src=\"../figs/BEST (Berkeley Earth Surface Temperature) 1880-2021_WSDI_ANN_TrendMap_1977-2021_24to51_-125to-65.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcaa08-c016-44e2-91a0-f78f7b15e2ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "If the trends above are robust long term trends, then we should see a similar result using more data years. If we use 100 total data years, do we see the same pattern of trends and similar areas of statistical significance over CONUS? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69bb013c-9ff2-4db9-be48-48ba5043626d",
   "metadata": {},
   "source": [
    "<img src=\"../figs/longterm/BEST (Berkeley Earth Surface Temperature) 1880-2021_WSDI_ANN_TrendMap_1922-2021_24to51_-125to-65.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b98eb-2142-43e4-acf4-9f63878afd61",
   "metadata": {},
   "source": [
    "The 100-year trend in WSDI is statistically significant for much of the country, except for a large hole across the Plains and Midwest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01c084-4377-4f64-be31-7b9b8fb6e740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
