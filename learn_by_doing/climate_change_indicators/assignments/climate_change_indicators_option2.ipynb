{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca70f373-08a8-4a42-b42b-a6b34785431a",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: Climate Change Indicators, Your Turn! Option 2 Answer Key\n",
    "\n",
    "**Developed By:** Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "**Original Development Date:** May 2024\n",
    "\n",
    "**Package Requirements:** xarray, netcdf4, numpy, pandas, scipy, matplotlib, jupyter, cartopy\n",
    "\n",
    "**Links:** **[OSF project link](https://osf.io/zhpd5/)**, [link to this notebook on github](https://github.com/kerriegeil/MSU_py_training/blob/main/learn_by_doing/climate_change_indicators/assignments/climate_change_indicators_option2.ipynb)\n",
    "\n",
    "---\n",
    "**Assignment:**\n",
    "\n",
    "Modify the TNx analysis to operate on gridded data (3-dimensional instead of the 1-dimensional data used in this notebook), performing all calculations at each grid cell. Include the relevant parts of the data cleaning section, calculate TNx, and then calculate the January TNx trend and pvalue. Present your results in a figure that shows the January TNx trend in C/year for each grid cell on a map if it is statistically significant at the 95% confidence level. Compare your results to climdex.org, looking at the trend in Jan TNx over CONUS for similar data years (1975-2019) and also for data years 1920-2019 from the Berkeley Earth Surface Temperature dataset.\n",
    "\n",
    "&emsp;Hints:\n",
    "- Use gridded tmin data for Mississippi from the file tmin_AgERA5_Mississippi_Daily_1979-2023.nc\n",
    "- For data cleaning, do the steps that apply to tn only\n",
    "    - nan for daily temperature greater than 70C or less than -70C\n",
    "    - remove leap days\n",
    "    - daily temperature outliers (5x std)\n",
    "    - you may also want to use the function get_nans_per_month to plot the maximum number of nans per month found at each grid cell\n",
    "- To use scipy.stats.linregress with multidimensional data, use **[xr.stack](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.stack.html)** and a for loop or use nested for loops. Alternatively, write your own function to vectorize calculation of linear trend and pvalue.\n",
    "- Plot the map of Jan trend values with units C/year, but only show the trend at grid cells that are statistically significant at the 95% confidence level or greater (use the p value and **[xarray.DataArray.where](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.where.html)** to mask out insignificant trends)\n",
    "- Draw state boundaries on your map with cartopy (import cartopy.crs as ccrs, import cartopy.feature as cf) or use a different method/package of your choice\n",
    "- Go to **[climdex.org](https://www.climdex.org/access/)** and get the png of Jan TNx trend over CONUS using data years 1975-2019 from the BEST dataset\n",
    "- Go to **[climdex.org](https://www.climdex.org/access/)** and get the png of Jan TNx trend over CONUS using data years 1920-2019 from the BEST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122b196-ae1a-4d98-a36a-fd60b811d82b",
   "metadata": {},
   "source": [
    "# Import packages and define workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a1f49-0d82-40f6-9127-5a5b0ae055f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import warnings\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4897bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for data downloads\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a559a-e07b-4923-b383-084121ffaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames to save data to\n",
    "tmin_f='../data/tmin_AgERA5_Mississippi_Daily_1979-2023.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56113f",
   "metadata": {},
   "source": [
    "# Obtaining the data\n",
    "\n",
    "We'll use the same AgERA5 dataset but this time we'll use data at all lat/lon grid points in Mississippi instead of just near Starkville. \n",
    "\n",
    "<br>\n",
    "<font color=\"green\"><b>\n",
    "**NOTE: You only need to run the following urlretrieve cell once. The data file will then be located on your computer. This file totals approximately 96MB in size.**\n",
    "</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876be141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmin for MS\n",
    "url='https://osf.io/xj8h3/download' # url to the data\n",
    "urlretrieve(url, tmin_f) # download and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d99cfe-8963-4f0b-95ce-1e03fd0f6aaa",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "For the Starkville data, we know that the suggested data cleaning didn't change our data arrays (except for dropping leap days). But now, instead of working with 1 Starkville timeseries, we're working with 53 latitudes * 40 longitudes = 2120 different points in space that all have their own timeseries (3 dimensional data time, lat, lon).\n",
    "\n",
    "We should do all data cleaning steps that apply to tn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0871a6-35da-400c-904a-91fa346ec22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = xr.open_dataarray(tmin_f)\n",
    "tn = tn.squeeze()\n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ad4c3-67ee-4014-a5ff-6e8be117e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### nan for daily temperature greater than 70C (158F) or less than -70C (-94F)\n",
    "# is tn>70C or tn<-70C?\n",
    "((tn>70)|(tn<-70)).data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30142921-9d25-482b-a4d3-bf9baca552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### drop leap days (i.e Feb 29th)\n",
    "\n",
    "# create a boolean array of dim 'time' where leap days are True and all other days are False\n",
    "isleapday=xr.where((tn.time.dt.day==29) & (tn.time.dt.month==2),True,False)\n",
    "\n",
    "# drop leap days\n",
    "tn=tn.where(~isleapday,drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd287-828c-4857-8a52-697181889c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the nan situation\n",
    "\n",
    "# a function that sums the number of nans in each month of data\n",
    "def get_nans_per_month(data_in):\n",
    "    month_groups=pd.MultiIndex.from_arrays([data_in.time['time.year'].data,data_in.time['time.month'].data])\n",
    "    data_in.coords['month_groups']=('time',month_groups)    \n",
    "    nancount=data_in.isnull().groupby('month_groups').sum()\n",
    "    return nancount\n",
    "\n",
    "nan_per_month=get_nans_per_month(tn.copy())\n",
    "\n",
    "# the maximum number of nans per month at each grid cell\n",
    "max_nan_per_month=nan_per_month.max('month_groups')\n",
    "\n",
    "# plot it\n",
    "fig=plt.figure(figsize=(5,5))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "max_nan_per_month.plot(cmap='summer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa1062-628b-4c06-9168-e584432f5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### daily temperature outliers \n",
    "\n",
    "# find the time-mean for each day of the year\n",
    "tn_daily_mean=tn.groupby(tn.time.dt.dayofyear).mean('time')\n",
    "\n",
    "# find the standard deviation for each day of the year\n",
    "# .std on a timeseries of all nan will throw a runtime warning\n",
    "# we can silence the warning with warnings.catch_warnings() and warnings.filterwarnings()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
    "    tn_stddev=tn.groupby(tn.time.dt.dayofyear).std('time')\n",
    "\n",
    "# define daily outlier temperature as exceeding the mean +/- 5 times standard deviation\n",
    "tn_outlier_upper, tn_outlier_lower=(tn_daily_mean+tn_stddev*5), (tn_daily_mean-tn_stddev*5)\n",
    "print('tn',(tn.groupby(tn.time.dt.dayofyear)>tn_outlier_upper).data.sum(), (tn.groupby(tn.time.dt.dayofyear)<tn_outlier_lower).data.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd0dc3-47e2-404b-9607-5dac5a42bba2",
   "metadata": {},
   "source": [
    "Turns out the only change we ended up making to our tn data array was dropping leap days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e228c7c-fee9-4609-a5db-9c1d024f5f91",
   "metadata": {},
   "source": [
    "# Monthly Maximum Value of Daily Minimum Temperature (TNx)\n",
    "\n",
    "- max(each month of daily minimum temperature values) for each grid cell\n",
    "\n",
    "Here we are inputting daily data and pulling out 1 value per month at each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204c5d4-8ba4-47d4-a892-0fc1c34b4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create monthly datetimes \n",
    "time_months=pd.date_range(tn.time.data[0],tn.time.data[-1],freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e6d98-f37b-4c8c-9c57-5ed662f96c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index value for every month in the timeseries\n",
    "month_groups=pd.MultiIndex.from_arrays([tn.time['time.year'].data,tn.time['time.month'].data])\n",
    "\n",
    "# add the month_groups index the time coordinate labels\n",
    "tn.coords['month_groups']=('time',month_groups)    \n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca41616-8ea5-4687-b1c6-b20577c98e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now groupby month and find the maximum value of each month\n",
    "TNx=tn.groupby('month_groups').max('time')\n",
    "TNx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9bcaf-8f47-4aad-8bd9-9266422152f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the month_groups dim name and coordinate labels to datetimes\n",
    "TNx=TNx.rename({'month_groups':'time'}) # renaming a coordinate and dimension\n",
    "TNx=TNx.drop_vars(['time_level_0','time_level_1']) # we don't need these leftovers\n",
    "TNx.coords['time']=('time',time_months) # replace coord labels \n",
    "TNx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ae4e0-3e41-4a3e-bfa1-d59840007969",
   "metadata": {},
   "source": [
    "Our TNx result is now 3-dimensional (time,lat,lon) instead of 1-dimensional like before when we were working with a single data point and had only the time dimension.\n",
    "\n",
    "Let's select some of our 3D TNx array to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d110c-0da9-4038-ad27-e5e130caa870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot TNx timeseries for a single point from the 3D array\n",
    "\n",
    "# using the first lat and lon\n",
    "lat=TNx.lat[0]\n",
    "lon=TNx.lon[0]\n",
    "\n",
    "# # or this way will yield the same as above\n",
    "# lat=TNx.lat.sel(lat=35.2,method='nearest')\n",
    "# lon=TNx.lon.sel(lon=-91.8,method='nearest')\n",
    "\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "TNx.sel(lat=lat,lon=lon).plot()\n",
    "plt.title(f'monthly maximum value of daily minimum temperature (TNx) at lat {lat:.2f} lon {lon:.2f}')\n",
    "plt.ylabel('degrees C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce400184-2387-484d-883a-054a3126d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the map of TNx for a single time\n",
    "\n",
    "# using January 2020\n",
    "ptime='2020-01'\n",
    "\n",
    "fig=plt.figure(figsize=(5,5))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "TNx.sel(time=ptime).plot(cbar_kwargs={'label':'degrees C'})\n",
    "plt.title('TNx for '+ptime)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439f8d3-ea5d-464c-bdd3-be7d92c883a0",
   "metadata": {},
   "source": [
    "# Trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c9802-73e5-480f-9084-fbb517924683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to only January\n",
    "\n",
    "TNx_jan=TNx.where(TNx.time.dt.month==1,drop=True)\n",
    "TNx_jan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ae3ca-5e46-41eb-8f1d-ada4bb92f21a",
   "metadata": {},
   "source": [
    "scipy.stats.linregress only operates on 1-dimensional data arrays so if we want to stick with this function to calculate the linear regression information, we will need to either use nested loops or .stack plus one loop. Note, it's best to avoid looping if you can in python, because it will be much slower than vectorized computation. Here, I will use .stack and a single loop to build arrays for trend and pval with scipy.stats.linregress \n",
    "\n",
    "However, if you were operating on a larger dataset (higher spatial resolution or global extent) this technique of calculating trend and pval may be too slow. At the end of this notebook, I'll show a custom function for linear regression on 3D xarray data array that doesn't stack or loop, which would be much faster on big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfce286-cab0-47e2-9997-118d8eadab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack lat and lon dimensions into a single dimension called space\n",
    "\n",
    "TNx_jan_stacked=TNx_jan.stack(space=['lat','lon'])\n",
    "TNx_jan_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f90e10-e5e3-4db2-a69c-5f3f6c423e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two arrays of dimension 'space' to hold our results and initialize to nan\n",
    "\n",
    "pval=TNx_jan_stacked.isel(time=0).copy()  # copy a single time from our stacked array\n",
    "pval[:]=np.nan  # initialize to nan\n",
    "del pval['time']  # get rid of the time coordinate label that isn't relevant\n",
    "\n",
    "trend=TNx_jan_stacked.isel(time=0).copy()\n",
    "trend[:]=np.nan\n",
    "del trend['time']\n",
    "\n",
    "pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f823b-bc37-44b8-9c49-eb93903b19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression with scipy using .stack and a for loop\n",
    "\n",
    "xvals=TNx_jan.time.dt.year\n",
    "\n",
    "for i,point in enumerate(pval.space):\n",
    "    reg_info=ss.linregress(xvals,TNx_jan_stacked.sel(space=point))\n",
    "    pval[i]=reg_info.pvalue\n",
    "    trend[i]=reg_info.slope # C/year     \n",
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd88d7d-3c08-4f9d-9cd1-a2c37fad1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now unstack the results back to two dimensions lat,lon\n",
    "\n",
    "pval=pval.unstack()\n",
    "trend=trend.unstack()\n",
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d2670-0e59-4d84-8efd-60c361baf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trend and pval results separately\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "trend.plot(cbar_kwargs={'label':'C/year'})\n",
    "plt.title('TNx Jan trend')\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "pval.plot()\n",
    "plt.title('TNx Jan trend p value')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cb97f-3f4c-404a-b77e-c1312d4bfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trend where statistical significance is at the 95% condifidence level or greater (using pval as a mask)\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "trend.where(pval<0.05).plot(cbar_kwargs={'label':'C/year'})\n",
    "plt.title('TNx Jan trend (p < 0.05)')\n",
    "# plt.savefig('figs/cci_option2_figure.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face2c33-b611-4d02-8228-f32859883012",
   "metadata": {},
   "source": [
    "# Trend analysis with custom regression function \n",
    "\n",
    "vectorized (multiple dimensions, no stacking, no looping) linear regression with p values \n",
    "\n",
    "This is the approach I would take with bigger data because loops are slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592dbed-9786-4291-92ba-a57d7e13546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(x,y):\n",
    "    \n",
    "    # Compute x length, and x and y mean and standard deviation\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    xmean = x.mean('time')\n",
    "    xstd = x.std('time') \n",
    "    \n",
    "    ymean = y.mean('time')\n",
    "    ystd = y.std('time')\n",
    "    \n",
    "    # here's where you would compute equivalent sample size\n",
    "    # if you wanted to account for autocorrelation\n",
    "    \n",
    "    # Compute covariance \n",
    "    # including min_count=1 will return nan instead of 0 at the ocean points\n",
    "    cov = ((x - xmean)*(y - ymean)).sum('time',min_count=1)/n\n",
    "\n",
    "    # Compute correlation\n",
    "    cor = cov/(xstd*ystd)\n",
    "\n",
    "    # Compute regression slope \n",
    "    slope = cov/(xstd**2)\n",
    "\n",
    "    # Compute t statistc and p-value\n",
    "    tstats = cor*np.sqrt(n-2)/np.sqrt(1-cor**2)\n",
    "    p = ss.t.sf(abs(tstats), n-2)*2 # *2 for two-sided test\n",
    "    p = xr.DataArray(p, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    return slope,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee8695-fba1-429f-8551-0395a0fc7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our custom function\n",
    "slope,p=linear_reg(TNx_jan.time.dt.year,TNx_jan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637847fc-8c9c-4b85-9186-ad86e351d8aa",
   "metadata": {},
   "source": [
    "Don't worry about the degrees of freedom warning. The ystd line throws this warning because a few of the grid cells in our data array are all nan (the ocean grid cells). If you want to turn off this warning you could use warnings.catch_warnings and warnings.filterwarnings like we did in the data cleaning section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9d794-7868-423a-b448-823e7e99be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trend and pval results separately\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "slope.plot(cbar_kwargs={'label':'C/year'})\n",
    "plt.title('TNx Jan trend')\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "p.plot()\n",
    "plt.title('TNx Jan trend p value')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd45fe4-6c43-4ee6-be77-0e6d83a69d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trend where statistical significance is at the 90% condifidence level or greater (using pval as a mask)\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.3)\n",
    "slope.where(p<0.05).plot(cbar_kwargs={'label':'C/year'})\n",
    "plt.title('TNx Jan trend (p < 0.05)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3c7e1-f9c0-418b-a08c-4b6f544f528d",
   "metadata": {},
   "source": [
    "# Difference betwee scipy.stats.linregress and custom regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f06e7c-74cd-40e6-9fe8-a0bf936bbe18",
   "metadata": {},
   "source": [
    "If we plot the difference in trend between our custom function and scipy.stats.linregress we would expect to see just noise (a random pattern of very small differences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab575cf1-50b1-4db7-95c5-5a1f7493932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(slope-trend).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321819f-2c20-406e-8bf6-26347d35fcfa",
   "metadata": {},
   "source": [
    "We should also see very small differences in p values from our custom function and scipy.stats.linregress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80644ec4-b8ee-4f0e-b3ae-d462a096ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(p-pval).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670f82f-5062-4652-8af4-f9d348e093b4",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e063874-e64e-484d-86e0-05bd5985655b",
   "metadata": {},
   "source": [
    "The trend in TNx is statisically significant at the 95% confidence level across all of Mississippi except for one small spot in extreme southest MS.\n",
    "\n",
    "We find a similar result for Mississippi from climdex.org, using BEST data over the years 1975-2019 to calculate TNx January trend."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47c81738-b5cd-43ac-acf8-265970b0a9ff",
   "metadata": {},
   "source": [
    "<img src=\"../figs/BEST (Berkeley Earth Surface Temperature) 1880-2019_TNx_JAN_TrendMap_1975-2019_24to51_-125to-65.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66281b6a-4427-48c8-afec-f9ca27e57eaa",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "However, if we look if we look at the 100-year trend (1920-2019) in Jan TNx we see no statistical significance over Mississippi. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0267862-5895-4ca1-9d30-7aecac9df1ad",
   "metadata": {},
   "source": [
    "<img src=\"../figs/longterm/BEST (Berkeley Earth Surface Temperature) 1880-2019_TNx_JAN_TrendMap_1920-2019_24to51_-125to-65.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d51a9-cf12-4691-a820-9b04c497c5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnbydoing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
