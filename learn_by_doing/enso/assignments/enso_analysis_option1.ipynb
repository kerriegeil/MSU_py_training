{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: ENSO, Your Turn! Option 1 Answer Key\n",
    "\n",
    "**Developed By:** Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "**Original Development Date:** June 2024\n",
    "\n",
    "**Package Requirements:** xarray, netcdf4, numpy, pandas, scipy, matplotlib, jupyter, geopandas\n",
    "\n",
    "**Links:** **[OSF project link](https://osf.io/zhpd5/)**, [link to this notebook on github]()\n",
    "\n",
    "---\n",
    "**Assignment:**\n",
    "\n",
    "Using a shapefile with country boundaries, show a table (use a pandas dataframe) of the percent area of each country in South America where there is statistically significant anomalous temperature and precipitation during strong El Nino and La Nina events. \n",
    "\n",
    "&emsp;Hints:\n",
    "- Use World_Countries_Generalized.shp for country boundaries and subset to South America. Here's the code to download the shapefile\n",
    "```\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# create a folder for data downloads\n",
    "if not os.path.exists('../data/World_Countries'):\n",
    "    os.makedirs('../data/World_Countries')\n",
    "\n",
    "# filenames to save data to and download urls\n",
    "base_filename='../data/World_Countries/World_Countries'\n",
    "\n",
    "shpfile_info=  {'.cpg':'https://osf.io/5xrgc/download',\n",
    "                '.dbf':'https://osf.io/3a6rp/download',\n",
    "                '.prj':'https://osf.io/43mnp/download',\n",
    "                '.shp':'https://osf.io/r4dez/download',\n",
    "                '.shp.xml':'https://osf.io/s4cvy/download',\n",
    "                '.shx':'https://osf.io/kp6cm/download'}    \n",
    "for ext,url in shpfile_info.items():\n",
    "    filename=base_filename+ext\n",
    "    print('downloading',filename)\n",
    "    urlretrieve(url,filename) # download and save data\n",
    "```\n",
    "- Repeat the appropriate steps from science questions 1 and 3\n",
    "- Use the country boundaries and your gridded results to quantify how much area of each country experiences statistically significant anomalous T and PR (use p<=0.1)\n",
    "- Format your results in a pandas dataframe. At minimum, your final dataframe should have columns for COUNTRY, T_PERCENT_AREA, PR_PERCENT_AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and define workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "\n",
    "import shapely\n",
    "import xagg\n",
    "# from cdo import *\n",
    "# cdo   = Cdo()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for data downloads\n",
    "if not os.path.exists('../data/World_Countries'):\n",
    "    os.makedirs('../data/World_Countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames to save data to and download urls\n",
    "base_filename='../data/World_Countries/World_Countries'\n",
    "\n",
    "shpfile_info=  {'.cpg':'https://osf.io/5xrgc/download',\n",
    "                '.dbf':'https://osf.io/3a6rp/download',\n",
    "                '.prj':'https://osf.io/43mnp/download',\n",
    "                '.shp':'https://osf.io/r4dez/download',\n",
    "                '.shp.xml':'https://osf.io/s4cvy/download',\n",
    "                '.shx':'https://osf.io/kp6cm/download'}\n",
    "\n",
    "# files we've already downloaded\n",
    "nino_f = '../data/nino34_anomalies_monthly_NOAA.txt'\n",
    "t_f = '../data/tavg_monthly_BerkeleyEarth.nc'\n",
    "pr_f = '../data/precip_monthly_PRECL_NOAA.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data\n",
    "\n",
    "We'll use a shapefile of country boundaries called [World Countries, originally obtained from ESRI ArcGIS Hub](https://hub.arcgis.com/datasets/esri::world-countries/explore) in June 2024 and copied to the [enso component of the MSU_py_training OSF project](https://osf.io/e726y/). \n",
    "\n",
    "<br>\n",
    "<font color=\"green\"><b>\n",
    "**NOTE: You only need to run the following urlretrieve cell once. The data files will then be located on your computer. Files total approximately 130MB in size.**\n",
    "</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download shapefile\n",
    "\n",
    "for ext,url in shpfile_info.items():\n",
    "    filename=base_filename+ext\n",
    "    print('downloading',filename)\n",
    "    urlretrieve(url,filename) # download and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "We'll copy over the data cleaning steps from the enso_analysis.ipynb as well as subset global country boundaries to South American countries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning copied from enso_analysis.ipynb\n",
    "\n",
    "year_start = '1948'\n",
    "year_end = '2023'\n",
    "base_start = '1981'\n",
    "base_end = '2010'\n",
    "\n",
    "dates=pd.date_range('1870-01-01','2024-12-01',freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning copied from enso_analysis.ipynb\n",
    "\n",
    "# Nino 3.4 data\n",
    "nino_raw=pd.read_csv(nino_f,sep=r'\\s+',skiprows=1,skipfooter=7,header=None,index_col=0,na_values=-99.99,engine='python')\n",
    "nino=nino_raw.to_numpy().flatten() \n",
    "nino=xr.DataArray(nino,name='nino',dims='time',coords={'time':dates}) \n",
    "nino.attrs['standard_name']='nino3.4 index'\n",
    "nino.attrs['units']='C'\n",
    "nino=nino.sel(time=slice(year_start,year_end))\n",
    "\n",
    "# precipitation data\n",
    "ds=xr.open_dataset(pr_f)\n",
    "pr=ds.precip\n",
    "pr=pr.sel(time=slice(year_start,year_end)) \n",
    "pr=pr.reindex(lat=pr.lat[::-1]) \n",
    "pr.coords['lon']=xr.where(pr.coords['lon']>180,pr.coords['lon']-360,pr.coords['lon'])\n",
    "roll_len=len(pr.lon)//2\n",
    "pr=pr.roll(lon=roll_len,roll_coords=True)\n",
    "pr_base=pr.sel(time=slice(base_start,base_end)) \n",
    "pr_clim=pr_base.groupby(pr_base.time.dt.month).mean('time') \n",
    "pr_anom=pr.groupby(pr.time.dt.month) - pr_clim  \n",
    "pr_anom.attrs['standard_name']='pr anomaly'\n",
    "pr_anom.attrs['units']='mm/day'\n",
    "\n",
    "# temperature data\n",
    "ds=xr.open_dataset(t_f)\n",
    "dates=pd.date_range('1750-01-01','2024-03-01',freq='MS')\n",
    "ds['time']=dates\n",
    "ds=ds.rename({'month_number':'month','latitude':'lat','longitude':'lon'})\n",
    "t_anom_5180=ds.temperature\n",
    "clim_5180=ds.climatology\n",
    "t=t_anom_5180.groupby(t_anom_5180.time.dt.month)+clim_5180\n",
    "t_base=t.sel(time=slice(base_start,base_end))  # subset in time\n",
    "clim_8110 = t_base.groupby(t_base.time.dt.month).mean('time')  # long term means for each month\n",
    "t_anom=t.groupby(t.time.dt.month)-clim_8110\n",
    "t_anom=t_anom.sel(time=slice(year_start,year_end))\n",
    "t_anom=t_anom.rename('tavg')\n",
    "t_anom.attrs['standard_name']='T anomaly'\n",
    "t_anom.attrs['units']='C'\n",
    "\n",
    "# check first and last time is the same for all data\n",
    "variables=[nino, pr_anom, t_anom] # list of arrays\n",
    "for var in variables:\n",
    "    print(var.name, var.time[0].data,var.time[-1].data)\n",
    "\n",
    "# clean up\n",
    "del ds, nino_raw, pr, pr_base, pr_clim,  t, t_anom_5180, t_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shapefile of global country boundaries\n",
    "countries=gpd.read_file(base_filename+'.shp')\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.crs\n",
    "# options:\n",
    "# 1) project both countries and data to equal area projections\n",
    "# 2) unproject countries and use the built in geopandas function to compute are on non-projected country polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying unproject first\n",
    "\n",
    "# subset to south america\n",
    "countries=countries.loc[(countries['CONTINENT']=='South America')\n",
    "                        &(countries['LAND_TYPE'].str.contains('Primary land'))].reset_index(drop=True)\n",
    "countries=countries.to_crs(\"epsg:4326\")\n",
    "countries\n",
    "\n",
    "# array(['Argentina', 'Aruba', 'Bolivia', 'Bonaire', 'Brazil', 'Chile',\n",
    "#        'Colombia', 'Curacao', 'Ecuador', 'Falkland Islands',\n",
    "#        'French Guiana', 'Guyana', 'Paraguay', 'Peru', 'Suriname',\n",
    "#        'Trinidad and Tobago', 'Uruguay', 'Venezuela'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # countries.crs.is_geographic\n",
    "# # not countries.crs\n",
    "# geod = countries.crs.get_geod()\n",
    "# geod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries.geometry[0].geoms[0].exterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shapely.geometry.polygon.orient(countries.geometry[0].geoms[0], 1)\n",
    "# # countries.geometry.geom_type#.crs#[9].geom_type\n",
    "# # all(countries.geometry.geom_type).str.contains(['MultiPolygon','Polygon'])\n",
    "# countries.geometry.geom_type.isin(['MultiPolygon','Polygon'])\n",
    "# all(countries.geometry.geom_type.isin(['MultiPolygon','Polygon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geod.geometry_area_perimeter(orient(geom, 1))[0]\n",
    "# test= countries.geometry[0].geoms[0]\n",
    "# test.exterior\n",
    "\n",
    "# geod = countries.crs.get_geod()\n",
    "# area_km2=[geod.geometry_area_perimeter(shapely.geometry.polygon.orient(geom,1))[0]/(1000^2) for geom in countries.geometry[0].geoms]\n",
    "# area_km2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry.polygon import orient\n",
    "def gpd_geographic_area(geodf):\n",
    "    if not geodf.crs and geodf.crs.is_geographic:\n",
    "        raise TypeError('geodataframe should have geographic coordinate system')\n",
    "        \n",
    "    geod = geodf.crs.get_geod()\n",
    "    def area_calc(geom):\n",
    "        if geom.geom_type not in ['MultiPolygon','Polygon']:\n",
    "            return np.nan\n",
    "        \n",
    "        # For MultiPolygon do each separately\n",
    "        if geom.geom_type=='MultiPolygon':\n",
    "            return np.sum([area_calc(p) for p in geom.geoms])/(1000^2)\n",
    "\n",
    "        # orient to ensure a counter-clockwise traversal. \n",
    "        # See https://pyproj4.github.io/pyproj/stable/api/geod.html\n",
    "        # geometry_area_perimeter returns (area, perimeter)\n",
    "        return geod.geometry_area_perimeter(orient(geom, 1))[0]/(1000^2)\n",
    "    \n",
    "    return geodf.geometry.apply(area_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries['area_km2']=gpd_geographic_area(countries)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function operates on a single polygon or multipolygon\n",
    "# # and can be applied to a geodataframe\n",
    "# def area_calc(geom):\n",
    "#     if not all(geom.geom_type.isin(['MultiPolygon','Polygon'])):\n",
    "#         raise TypeError('some geometries are not polygon or multipolygon')\n",
    "\n",
    "#     geod = geom.crs.get_geod()        \n",
    "#     if geom.geom_type=='MultiPolygon':\n",
    "#         # For multipolygon do each polygon separately and sum\n",
    "#         return np.sum([geod.geometry_area_perimeter(shapely.geometry.polygon.orient(p,1))[0]/(1000^2) for p in geom.geoms])\n",
    "#     else:\n",
    "#         return geod.geometry_area_perimeter(shapely.geometry.polygon.orient(geom,1))[0]/(1000^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # geod = countries.crs.get_geod()\n",
    "# countries.geometry.apply(area_calc)\n",
    "# # countries.apply(area_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get country areas\n",
    "# geod = countries.crs.get_geod()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from shapely.geometry.polygon import orient\n",
    "# def gpd_geographic_area(geodf):\n",
    "#     if not geodf.crs and geodf.crs.is_geographic:\n",
    "#         raise TypeError('geodataframe should have geographic coordinate system')\n",
    "        \n",
    "#     geod = geodf.crs.get_geod()\n",
    "#     def area_calc(geom):\n",
    "#         if geom.geom_type not in ['MultiPolygon','Polygon']:\n",
    "#             return np.nan\n",
    "        \n",
    "#         # For MultiPolygon do each separately\n",
    "#         if geom.geom_type=='MultiPolygon':\n",
    "#             return np.sum([area_calc(p) for p in geom.geoms])\n",
    "\n",
    "#         # orient to ensure a counter-clockwise traversal. \n",
    "#         # See https://pyproj4.github.io/pyproj/stable/api/geod.html\n",
    "#         # geometry_area_perimeter returns (area, perimeter)\n",
    "#         return geod.geometry_area_perimeter(orient(geom, 1))[0]\n",
    "    \n",
    "#     return geodf.geometry.apply(area_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count how many different countries and see their names\n",
    "# print(len(countries.COUNTRY.unique()))\n",
    "# countries.COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Main Analysis\n",
    "\n",
    "First, we need to know when El Nino and La Nina events occurred so we'll copy over code from question 1 in enso_analysis.ipynb (How many strong El Nino and La Nina events have occurred from 1948 to 2023?). We only need the code that creates the array `nino_events` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from question 1 in enso_analysis.ipynb\n",
    "\n",
    "# constants based on our criteria\n",
    "nmonths=5\n",
    "event_thresh=0.6\n",
    "\n",
    "# first calculate the 5-month rolling mean\n",
    "nino_rollmean=nino.rolling(time=nmonths,center=True).mean()\n",
    "\n",
    "# create an array to hold our results and initialize to nan\n",
    "# this array is where we will fill values with +1,-1\n",
    "nino_events=nino_rollmean.copy() \n",
    "nino_events[:]=np.nan\n",
    "\n",
    "# now loop through months and fill +1, -1 for windows of 5 months that meet our criteria\n",
    "for i,value in enumerate(nino_rollmean):\n",
    "    # La Nina conditions\n",
    "    if  value < -event_thresh:\n",
    "        # possible La Nina conditions, look forward 4 more months\n",
    "        window=nino_rollmean[i:i+nmonths]\n",
    "        if all(window < -event_thresh):\n",
    "            nino_events[i:i+nmonths] = -1\n",
    "\n",
    "    # El Nino conditions\n",
    "    if  value > event_thresh:\n",
    "        # possible El Nino conditions, look forward 4 more months\n",
    "        window=nino_rollmean[i:i+nmonths]\n",
    "        if all(window > event_thresh):\n",
    "            nino_events[i:i+nmonths]=1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to build our El Nino composite and determine statistical significance. We'll copy over the relevant code from question 3 in enso_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from question 3 in enso_analysis.ipynb\n",
    "\n",
    "# starting with el nino conditions, temperature\n",
    "# get temperature anomalies only for times during strong el nino conditions\n",
    "t_nino=t_anom.where(nino_events==1,drop=True)\n",
    "\n",
    "# now separate out winter DJF months\n",
    "# this is sample 1: winter months during strong el nino conditions\n",
    "t_nino_DJF=t_nino.groupby(t_nino.time.dt.season)['DJF'] \n",
    "\n",
    "# make a composite\n",
    "t_nino_DJF_composite=t_nino_DJF.mean('time',keep_attrs=True)\n",
    "\n",
    "# create a t sample that include all winter months DJF when there are not strong el nino conditions\n",
    "\n",
    "# all months that don't fall in strong nino events\n",
    "t_other=t_anom.where(nino_events!=1,drop=True) \n",
    "\n",
    "# pull out just DJF months\n",
    "# this is sample 2: all winter months that are NOT during strong el nino conditions\n",
    "t_other_DJF=t_other.groupby(t_other.time.dt.season)['DJF'] \n",
    "\n",
    "print('t nino and non-nino sample sizes:',t_nino_DJF.shape[0],t_other_DJF.shape[0]) \n",
    "\n",
    "# t-test for difference in means \n",
    "t_sigtest = ss.ttest_ind(t_nino_DJF, t_other_DJF, axis=0, equal_var=False)\n",
    "# numpy --> xarray\n",
    "t_nino_pval = xr.DataArray(t_sigtest.pvalue, coords={'lat':('lat',t_nino.coords['lat'].data),'lon':('lon',t_nino.coords['lon'].data)})\n",
    "\n",
    "\n",
    "# plot el nino temperature anomalies where statistically significant at 90% level\n",
    "pval=0.1\n",
    "\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "t_nino_DJF_composite.where(t_nino_pval<pval).plot(cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean temperature anomalies\\n during strong El Nino conditions (p < {pval})')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our shapefile to answer the question: which South American countries experience statistically significant anomalous temperature and precipitation over at least 50% of the country's area during strong El Nino? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_nino_DJF_composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a pandas dataframe from our results t_nino_DJF_composite and t_nino_pval\n",
    "\n",
    "# convert each xarray data array object to a pandas data frame\n",
    "t_nino_DJF_composite_df=t_nino_DJF_composite.to_dataframe().reset_index(level=[0,1])\n",
    "\n",
    "t_nino_pval.name='pval'\n",
    "t_nino_pval_df=t_nino_pval.to_dataframe().reset_index(level=[0,1])\n",
    "\n",
    "# # merge pvalues into t_nino_DJF_composite_df\n",
    "t_nino_DJF_composite_df=t_nino_DJF_composite_df.merge(t_nino_pval_df, how='left',on=['lat','lon'])\n",
    "t_nino_DJF_composite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_nino_DJF_composite_df.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the xagg package to compute the overlaps between grid cell polygons and country polygons\n",
    "# pixel_overlaps computes the relative area of overlap for each grid cell polygon\n",
    "# it takes an xarray dataset and a geopandas dataframe as inputs\n",
    "# see https://xagg.readthedocs.io/en/latest/xagg.html#xagg.wrappers.pixel_overlaps\n",
    "# and https://xagg.readthedocs.io/en/latest/xagg.html#xagg.core.get_pixel_overlaps\n",
    "weightmap = xagg.pixel_overlaps(t_nino_DJF_composite,countries,subset_bbox=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xa.pixel_overlaps function returns an object that contains \n",
    "# 1) a pandas dataframe with the grid cell polygon overlap information, \n",
    "# 2) a dictionary containing the xarray data array source grid info, \n",
    "# 3) a pandas series of geometry objects containing the geopandas source geometry info\n",
    "\n",
    "# access the dataframe with .agg\n",
    "# weightmap.agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xagg.pixel_overlaps calculates how much relative area of a country polygon that each grid cell that intersects the country occupies. The column rel_area contains this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relative area of each pixel intersecting the first polygon/mulipolygon (Argentina)\n",
    "# would be indexed like this, which returns a pandas series of 351 values\n",
    "# in this case 351 grid cells of our xarray data array intersect the Argentina polygon\n",
    "# weightmap.agg['rel_area'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the coordinates that correspond to the rel_area values (grid cell polygon centroids)\n",
    "# would be indexed like this, which returns a list of 351 (lat,lon) tuples\n",
    "# weightmap.agg['coords'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap.agg#.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each country we want to create a dataframe where each row contains data for a single grid cell: relative area, lat, lon values, data value, pvalue. We'll start with the first country Argentina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates lat,lon to dataframe\n",
    "df=pd.DataFrame(weightmap.agg['coords'][11],columns=['lat','lon'])\n",
    "\n",
    "# add rel_area as a dataframe column\n",
    "df['rel_area']=weightmap.agg['rel_area'][11][0].reset_index(drop=True)\n",
    "\n",
    "# join the tavg and pval info\n",
    "df=df.merge(t_nino_DJF_composite_df, how='left',on=['lat','lon'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xagg.diag.diag_fig(weightmap,0,t_nino_DJF_composite)\n",
    "\n",
    "# test=xagg.fix_ds(t_nino_DJF_composite)\n",
    "# weightmap.diag_fig(0,test)\n",
    "\n",
    "# grid_polygon_info = xagg.subset_find(t_nino_DJF_composite,weightmap.source_grid)\n",
    "# weightmap.diag_fig(1,grid_polygon_info)\n",
    "# grid_polygon_info\n",
    "\n",
    "weightmap.diag_fig(11,t_nino_DJF_composite.to_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_subset=t_nino_DJF_composite.sel(lat=slice(-20,0),lon=slice(-82,-68))\n",
    "data_subset=t_nino_DJF_composite.sel(lat=slice(9.5,11.5),lon=slice(-62,-60))\n",
    "# data_subset\n",
    "fig=plt.figure(figsize=(6,6))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "\n",
    "# Make a copy\n",
    "cmap = plt.colormaps.get_cmap(\"RdBu_r\").copy()\n",
    "\n",
    "# Choose the color\n",
    "cmap.set_bad('grey')\n",
    "\n",
    "data_subset.plot(cmap=cmap)#,cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "\n",
    "# Gridlines\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=['x','y','bottom','left'],\n",
    "                    linewidth=1, color='dimgray', alpha=0.5, linestyle=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the relative area of the country where nino anomalies meet the 90% confidence level\n",
    "area_impacted=df.loc[df['pval']<=0.1].rel_area.sum()\n",
    "area_impacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the result for all countries we can write a function and loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_area_impacted(coords,rel_area,xr_pval):\n",
    "    df=pd.DataFrame(coords,columns=['lat','lon'])\n",
    "    df['rel_area']=rel_area.reset_index(drop=True)\n",
    "    \n",
    "    xr_pval.name='pval'\n",
    "    pval_df=xr_pval.to_dataframe().reset_index(level=[0,1])\n",
    "    df=df.merge(pval_df, how='left',on=['lat','lon'])\n",
    "\n",
    "    area_impacted=df.loc[df['pval']<=0.1].rel_area.sum()*100.\n",
    "    area_impacted = round(area_impacted)\n",
    "    return area_impacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "for index,row in weightmap.agg.iterrows():\n",
    "    answer=percent_area_impacted(row.coords,row.rel_area[0],t_nino_pval)\n",
    "    results[row.COUNTRY]=answer\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side note about calculating irregular polygon areas from gridded/raster data\n",
    "\n",
    "Here, our gridded data is nan over ocean grids (or grids that are mostly ocean). This means a grid cell with a value of nan could slighly overlap where land is present. We have to take that into consideration when interpreting the accuracy/precision of these results.\n",
    "\n",
    "Take, for example, Trinidad and Tobago. The area we calculated is 98% instead of 100% because of the way the grid cells align with the coastline. This will be the case for any country with a coast.\n",
    "\n",
    "Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using xagg's diagnostic figure to look at which grid cells \n",
    "# overlap Trinidad and Tobago (row 11 of countries dataframe)\n",
    "weightmap.diag_fig(11,t_nino_DJF_composite.to_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a plot to look at which grid cells of our data are considered ocean (nan)\n",
    "\n",
    "fig=plt.figure(figsize=(6,6))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "# color nan grey\n",
    "cmap = plt.colormaps.get_cmap(\"RdBu_r\").copy()\n",
    "cmap.set_bad('grey') \n",
    "\n",
    "# subset based on the above plot's lat and lon\n",
    "t_nino_DJF_composite.sel(lat=slice(9.5,11.5),lon=slice(-62,-60)).plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the dictionary with .source_grid\n",
    "weightmap.source_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_test=pr_anom.isel(time=0).to_dataset()\n",
    "pr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdf=pr_test.stack(cell=('lat','lon')).to_dataframe()#.to_pandas()#.drop_vars(['lat','lon'])\n",
    "testdf=pr_test.to_dataframe().reset_index(level=[0,1])#.to_pandas()#.drop_vars(['lat','lon'])\n",
    "\n",
    "# testdf=testdf.reindex()\n",
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap = xa.pixel_overlaps(pr_test,countries,subset_bbox=False)\n",
    "weightmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_area=weightmap.agg['rel_area']\n",
    "rel_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords=weightmap.agg['coords']\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relative area of each pixel intersecting the first polygon/mulipolygon\n",
    "# would be indexed like this, which returns a pandas series of 351 values\n",
    "rel_area[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the coordinates that correspond to the rel_area valuse/pixels\n",
    "# would be indexed like this, which returns a list of 351 (lat,lon) tuples\n",
    "coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each polygon, we can create a df of pixel rel_area, center coordinates, and data value like this\n",
    "df=pd.DataFrame(coords[0],columns=['lat','lon'])\n",
    "df['rel_area']=rel_area[0][0]\n",
    "df=df.merge(testdf, how='left',on=['lat','lon'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the area where the data variable meets some criteria \n",
    "# for example, the magnitude of the pr anomaly gt 0.5\n",
    "\n",
    "df_sub=df[abs(df['precip'])>=0.5]\n",
    "relarea_criteria=df_sub['rel_area'].sum()\n",
    "relarea_criteria\n",
    "\n",
    "# now multiply by the area of the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peru=countries.loc[9]\n",
    "peru=countries[countries.COUNTRY=='Peru']\n",
    "# peru\n",
    "weightmap2 = xa.pixel_overlaps(pr_anom_ds,peru,subset_bbox=False)\n",
    "weightmap2.agg['rel_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap2.agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap2.agg.rel_area[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(weightmap2.agg.rel_area[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(weightmap2.agg.coords[0]).shape#.rel_area[0][0]\n",
    "\n",
    "weightmap2.agg.coords[0]#[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Querying polygon by column of the polygon `gdf`\n",
    "# weightmap.diag_fig({'name':'Alaska'},ds)\n",
    "\n",
    "# Plotting the first polygon in the polygon `gdf`\n",
    "weightmap.diag_fig(0,pr_anom_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rio\n",
    "pr_anom_ds=pr_anom.to_dataset()\n",
    "# pr_anom_ds=pr_anom_ds.rename({'lat':'y','lon':'x'})\n",
    "# pr_anom_ds=pr_anom_ds.rio.write_crs('epsg:4326')\n",
    "# pr_test=pr_anom_ds.rio.reproject('epsg:3857')\n",
    "pr_test=pr_anom_ds\n",
    "pr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_test=pr_test.rename({'y':'lat','x':'lon'})\n",
    "pr_test=xa.auxfuncs.get_bnds(pr_test)\n",
    "pr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_poly=xa.core.create_raster_polygons(pr_test)\n",
    "# grid_poly,grid_dict=xa.core.create_raster_polygons(pr_test) # doesn't work this way\n",
    "\n",
    "gdf_pixels=grid_poly['gdf_pixels']\n",
    "gdf_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_pixels.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.cos(np.deg2rad(pr_anom.lat))\n",
    "weights.name = \"weights\"\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_weighted = pr_anom.weighted(weights)\n",
    "pr_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdo gridarea yourdata.nc gridarea.nc \n",
    "ds=xr.open_dataset(pr_f)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnbydoing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
