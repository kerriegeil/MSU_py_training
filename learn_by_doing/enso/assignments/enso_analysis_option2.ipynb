{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: ENSO, Your Turn! Option 2 Answer Key\n",
    "\n",
    "**Developed By:** Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "**Original Development Date:** July 2024\n",
    "\n",
    "**Package Requirements:** xarray, netcdf4, numpy, pandas, scipy, matplotlib, cartopy, jupyter, geopandas, shapely, xagg\n",
    "\n",
    "**Links:** **[OSF project link](https://osf.io/zhpd5/)**, [link to this notebook on github](https://github.com/kerriegeil/MSU_py_training/blob/main/learn_by_doing/enso/assignments/enso_analysis_option2.ipynb)\n",
    "\n",
    "---\n",
    "**Assignment:**\n",
    "\n",
    "This specific assignment was requested by a past python learn by doing workshop participant. Thank you, it's great to have suggestions! \n",
    "\n",
    "Using a shapefile with country boundaries, show a table (use a pandas dataframe) of the percent area of each country in South America where there is statistically significant anomalous temperature during strong El Nino events. Also show the mean anomaly by country.\n",
    "\n",
    "Pay attention to the hints below, as this assignment uses data, packages, and coding techniques that weren't covered in the main notebook. As such, this is more of a learning assignment than a practice assignment. \n",
    "\n",
    "\n",
    "&emsp;Hints:\n",
    "- Use World_Countries_Generalized.shp for country boundaries and subset to South America. Here's the code to download the shapefile\n",
    "\n",
    "```python\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# create a folder for data downloads\n",
    "if not os.path.exists('../data/World_Countries'):\n",
    "    os.makedirs('../data/World_Countries')\n",
    "\n",
    "# filenames to save data to and download urls\n",
    "base_filename='../data/World_Countries/World_Countries'\n",
    "\n",
    "shpfile_info=  {'.cpg':'https://osf.io/5xrgc/download',\n",
    "                '.dbf':'https://osf.io/3a6rp/download',\n",
    "                '.prj':'https://osf.io/43mnp/download',\n",
    "                '.shp':'https://osf.io/r4dez/download',\n",
    "                '.shp.xml':'https://osf.io/s4cvy/download',\n",
    "                '.shx':'https://osf.io/kp6cm/download'}    \n",
    "for ext,url in shpfile_info.items():\n",
    "    filename=base_filename+ext\n",
    "    print('downloading',filename)\n",
    "    urlretrieve(url,filename) # download and save data\n",
    "\n",
    "```\n",
    "- Use geopandas to read the shapefile and subset the rows to South American countries with `LAND_TYPE` of `Primary Land`.\n",
    "- Calculate the area of each country in square kilometers and add that information to the geodataframe as a new column. The most accurate way to calculate areas of polygons spread across the globe is to do the calculations on an ellipsoid. You would use data that is in the geographic coordinate reference system epsg:4326 (not projected) and use the corresponding ellipsoid parameters. The other way to calculate shape areas which would be an acceptable substitute here is to project the geodataframe to an equal area projection (epsg: 6933), then use geopandas `.area` function on the geodataframe, and divide by 1000^2 to get km2. Here is a function that will do the calculation on an ellipsoid.\n",
    "```python\n",
    "\n",
    "def area_calc(geodf):\n",
    "    # input geodf is a geodataframe with crs epsg:4326\n",
    "\n",
    "    # function that operates on individual geometry objects\n",
    "    def get_area(geom,geod):        \n",
    "        if geom.geom_type not in ['MultiPolygon','Polygon']:\n",
    "            return np.nan\n",
    "        \n",
    "        # orient to ensure a counter-clockwise traversal. \n",
    "        # See https://pyproj4.github.io/pyproj/stable/api/geod.html\n",
    "        # geometry_area_perimeter returns (area, perimeter)\n",
    "        if geom.geom_type == 'Polygon':\n",
    "            return geod.geometry_area_perimeter(orient(geom, 1))[0]/1E6\n",
    "        # For MultiPolygon do each separately and sum\n",
    "        if geom.geom_type == 'MultiPolygon':\n",
    "            return np.sum([get_area(poly,geod) for poly in geom.geoms])\n",
    "\n",
    "    # check presence of geographic crs and execute or raise error\n",
    "    if geodf.crs and geodf.crs.is_geographic:\n",
    "        # apply the get_area function to each country (row)\n",
    "        geod = geodf.crs.get_geod()\n",
    "        geodf['AREA_KM2']=geodf.apply(lambda row : get_area(row.geometry,geod),axis=1)\n",
    "        return geodf\n",
    "    else:\n",
    "        raise TypeError('geodataframe should have geographic coordinate reference system') \n",
    "            \n",
    "```\n",
    "\n",
    "- From enso_analysis.ipynb, copy the appropriate data cleaning steps for the nino index `nino` and t anomaly `t_anom` data\n",
    "- From enso_analysis.ipynb, copy the appropriate analysis steps from science questions 1 and 3 to get `t_nino_DJF_composite` and `t_nino_pval` \n",
    "- To get the area impacted use the xagg package (after creating the weightmap, recommend trying the remaining steps for one country first, then creating a function to apply it to all countries):\n",
    "    - use `weightmap = xagg.pixel_overlaps(input, input, subset_bbox=False)` to compute the overlaps between all grid cell polygons and all country polygons\n",
    "    - for each country, use the weightmap from above to create a country-specific pandas dataframe `country_df` where each row contains information for a single grid cell that overlaps the country polygon. You'll have columns for lat and lon (use `weightmap.agg['coords'][indexrow]`) and rel_area (use `weightmap.agg['rel_area'][indexrow][0]`) to pull the relevant information from the weightmap.\n",
    "    - convert `t_nino_DJF_composite` and `t_nino_pval` to dataframes with columns for lat and lon (use `.reset_index(level=[0,1])`)\n",
    "    - use pandas `.merge` to merge the temperature anomaly data and pvalue into the country_df (use parameters `how='left',on=['lat','lon']`)\n",
    "    - use pandas `.loc` to sum rel_area where pval <= 0.1\n",
    "    - if you haven't already, create a function that takes country-specific info from the weightmap and outputs area_impacted. Then, generate results for all countries using a for loop.\n",
    "- To get the country-mean temperature anomaly use `xagg.core.aggregate()` along with the weightmap generated in the previous steps\n",
    "- Optional: calculate the country-mean temperature anomaly only where pval<=0.1. Hint: input a set of weights to xagg.pixel_overlaps() where insignificant pixels have weight 0 and significant pixels have weight 1.\n",
    "- Format your results into a pandas dataframe. Your final dataframe should have columns for `COUNTRY`, `T_PERCENT_AREA`, and `MEAN_T_ANOMALY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and define workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry.polygon import orient\n",
    "import xarray as xr\n",
    "import scipy.stats as ss\n",
    "import xagg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for data downloads\n",
    "if not os.path.exists('../data/World_Countries'):\n",
    "    os.makedirs('../data/World_Countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames to save data to and download urls\n",
    "base_filename='../data/World_Countries/World_Countries'\n",
    "\n",
    "shpfile_info=  {'.cpg':'https://osf.io/5xrgc/download',\n",
    "                '.dbf':'https://osf.io/3a6rp/download',\n",
    "                '.prj':'https://osf.io/43mnp/download',\n",
    "                '.shp':'https://osf.io/r4dez/download',\n",
    "                '.shp.xml':'https://osf.io/s4cvy/download',\n",
    "                '.shx':'https://osf.io/kp6cm/download'}\n",
    "\n",
    "# files we've already downloaded\n",
    "nino_f = '../data/nino34_anomalies_monthly_NOAA.txt'\n",
    "t_f = '../data/tavg_monthly_BerkeleyEarth.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the data\n",
    "\n",
    "We'll use a shapefile of country boundaries called [World Countries, originally obtained from ESRI ArcGIS Hub](https://hub.arcgis.com/datasets/esri::world-countries/explore) in June 2024 and copied to the [enso component of the MSU_py_training OSF project](https://osf.io/e726y/). \n",
    "\n",
    "<br>\n",
    "<font color=\"green\"><b>\n",
    "**NOTE: You only need to run the following urlretrieve cell once. The data files will then be located on your computer. Files total approximately 130MB in size.**\n",
    "</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download shapefile\n",
    "\n",
    "for ext,url in shpfile_info.items():\n",
    "    filename=base_filename+ext\n",
    "    print('downloading',filename)\n",
    "    urlretrieve(url,filename) # download and save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "We'll subset global country boundaries to South American countries and calculate country areas, as well as copy over the relevant data cleaning steps from enso_analysis.ipynb.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shapefile of global country boundaries\n",
    "countries=gpd.read_file(base_filename+'.shp')\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the crs\n",
    "countries.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to south america\n",
    "countries=countries.loc[(countries['CONTINENT']=='South America')\n",
    "                        &(countries['LAND_TYPE'].str.contains('Primary land'))].reset_index(drop=True)\n",
    "\n",
    "# showing how to use equal area projection to get country areas\n",
    "# for global data this may be less accurate than calculating areas with unprojected data on an ellipsoid\n",
    "# but I'm showing it because it is much simpler\n",
    "countries=countries.to_crs(\"epsg:6933\")\n",
    "countries['AREA_6933'] = countries.area/(1E6)\n",
    "\n",
    "# reproject to a geographic crs\n",
    "countries=countries.to_crs(\"epsg:4326\")\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate shape areas on an ellipsoid (geographic crs)\n",
    "\n",
    "def area_calc(geodf):\n",
    "    # input geodf is a geodataframe with crs epsg:4326\n",
    "    # function that operates on individual geometry objects\n",
    "    def get_area(geom,geod):        \n",
    "        if geom.geom_type not in ['MultiPolygon','Polygon']:\n",
    "            return np.nan\n",
    "        \n",
    "        # orient to ensure a counter-clockwise traversal. \n",
    "        # See https://pyproj4.github.io/pyproj/stable/api/geod.html\n",
    "        # geometry_area_perimeter returns (area, perimeter)\n",
    "        if geom.geom_type == 'Polygon':\n",
    "            return geod.geometry_area_perimeter(orient(geom, 1))[0]/1E6\n",
    "        # For MultiPolygon do each separately and sum\n",
    "        if geom.geom_type == 'MultiPolygon':\n",
    "            return np.sum([get_area(poly,geod) for poly in geom.geoms])\n",
    "\n",
    "    # check presence of geographic crs and execute or raise error\n",
    "    if geodf.crs and geodf.crs.is_geographic:\n",
    "        # apply the get_area function to each country (row)\n",
    "        geod = geodf.crs.get_geod()\n",
    "        geodf['AREA_KM2']=geodf.apply(lambda row : get_area(row.geometry,geod),axis=1)\n",
    "        return geodf\n",
    "    else:\n",
    "        raise TypeError('geodataframe should have geographic coordinate reference system')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries=area_calc(countries)\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning copied from enso_analysis.ipynb\n",
    "\n",
    "year_start = '1948'\n",
    "year_end = '2023'\n",
    "base_start = '1981'\n",
    "base_end = '2010'\n",
    "\n",
    "dates=pd.date_range('1870-01-01','2024-12-01',freq='MS')\n",
    "\n",
    "# Nino 3.4 data\n",
    "nino_raw=pd.read_csv(nino_f,sep=r'\\s+',skiprows=1,skipfooter=7,header=None,index_col=0,na_values=-99.99,engine='python')\n",
    "nino=nino_raw.to_numpy().flatten() \n",
    "nino=xr.DataArray(nino,name='nino',dims='time',coords={'time':dates}) \n",
    "nino.attrs['standard_name']='nino3.4 index'\n",
    "nino.attrs['units']='C'\n",
    "nino=nino.sel(time=slice(year_start,year_end))\n",
    "\n",
    "# temperature data\n",
    "ds=xr.open_dataset(t_f)\n",
    "dates=pd.date_range('1750-01-01','2024-03-01',freq='MS')\n",
    "ds['time']=dates\n",
    "ds=ds.rename({'month_number':'month','latitude':'lat','longitude':'lon'})\n",
    "t_anom_5180=ds.temperature\n",
    "clim_5180=ds.climatology\n",
    "t=t_anom_5180.groupby(t_anom_5180.time.dt.month)+clim_5180\n",
    "t_base=t.sel(time=slice(base_start,base_end))  # subset in time\n",
    "clim_8110 = t_base.groupby(t_base.time.dt.month).mean('time')  # long term means for each month\n",
    "t_anom=t.groupby(t.time.dt.month)-clim_8110\n",
    "t_anom=t_anom.sel(time=slice(year_start,year_end))\n",
    "t_anom=t_anom.rename('tavg')\n",
    "t_anom.attrs['standard_name']='T anomaly'\n",
    "t_anom.attrs['units']='C'\n",
    "\n",
    "# check first and last time is the same for all data\n",
    "variables=[nino, t_anom] # list of arrays\n",
    "for var in variables:\n",
    "    print(var.name, var.time[0].data,var.time[-1].data)\n",
    "\n",
    "# clean up\n",
    "del ds, nino_raw, t, t_anom_5180, t_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Main Analysis\n",
    "\n",
    "First, we need to know when El Nino and La Nina events occurred so we'll copy over code from question 1 in enso_analysis.ipynb (How many strong El Nino and La Nina events have occurred from 1948 to 2023?). We only need the code that creates the array `nino_events` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from question 1 in enso_analysis.ipynb\n",
    "\n",
    "# constants based on our criteria\n",
    "nmonths=5\n",
    "event_thresh=0.6\n",
    "\n",
    "# first calculate the 5-month rolling mean\n",
    "nino_rollmean=nino.rolling(time=nmonths,center=True).mean()\n",
    "\n",
    "# create an array to hold our results and initialize to nan\n",
    "# this array is where we will fill values with +1,-1\n",
    "nino_events=nino_rollmean.copy() \n",
    "nino_events[:]=np.nan\n",
    "\n",
    "# now loop through months and fill +1, -1 for windows of 5 months that meet our criteria\n",
    "for i,value in enumerate(nino_rollmean):\n",
    "    # La Nina conditions\n",
    "    if  value < -event_thresh:\n",
    "        \n",
    "        # possible La Nina conditions, look forward 4 more months\n",
    "        window=nino_rollmean[i:i+nmonths]\n",
    "        if all(window < -event_thresh):\n",
    "            nino_events[i:i+nmonths] = -1\n",
    "\n",
    "    # El Nino conditions\n",
    "    if  value > event_thresh:\n",
    "        # possible El Nino conditions, look forward 4 more months\n",
    "        window=nino_rollmean[i:i+nmonths]\n",
    "        if all(window > event_thresh):\n",
    "            nino_events[i:i+nmonths]=1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to build our El Nino temperature anomaly composite and determine statistical significance. We'll copy over the relevant code from question 3 in enso_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from question 3 in enso_analysis.ipynb\n",
    "\n",
    "# starting with el nino conditions, temperature\n",
    "# get temperature anomalies only for times during strong el nino conditions\n",
    "t_nino=t_anom.where(nino_events==1,drop=True)\n",
    "\n",
    "# now separate out winter DJF months\n",
    "# this is sample 1: winter months during strong el nino conditions\n",
    "t_nino_DJF=t_nino.groupby(t_nino.time.dt.season)['DJF'] \n",
    "\n",
    "# make a composite\n",
    "t_nino_DJF_composite=t_nino_DJF.mean('time',keep_attrs=True)\n",
    "\n",
    "# create a t sample that include all winter months DJF when there are not strong el nino conditions\n",
    "\n",
    "# all months that don't fall in strong nino events\n",
    "t_other=t_anom.where(nino_events!=1,drop=True) \n",
    "\n",
    "# pull out just DJF months\n",
    "# this is sample 2: all winter months that are NOT during strong el nino conditions\n",
    "t_other_DJF=t_other.groupby(t_other.time.dt.season)['DJF'] \n",
    "\n",
    "print('t nino and non-nino sample sizes:',t_nino_DJF.shape[0],t_other_DJF.shape[0]) \n",
    "\n",
    "# t-test for difference in means \n",
    "t_sigtest = ss.ttest_ind(t_nino_DJF, t_other_DJF, axis=0, equal_var=False)\n",
    "# numpy --> xarray\n",
    "t_nino_pval = xr.DataArray(t_sigtest.pvalue, coords={'lat':('lat',t_nino.coords['lat'].data),'lon':('lon',t_nino.coords['lon'].data)})\n",
    "\n",
    "\n",
    "# plot el nino temperature anomalies where statistically significant at 90% level\n",
    "pval=0.1\n",
    "\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "t_nino_DJF_composite.where(t_nino_pval<pval).plot(cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean temperature anomalies\\n during strong El Nino conditions (p < {pval})')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our shapefile to answer the question: what percent area of each country in South America experiences statistically significant anomalous temperature during strong winter El Nino events?\n",
    "\n",
    "First, we use the xagg package to compute the overlaps between grid cell polygons and country polygons.\n",
    "\n",
    "`xagg.pixel_overlaps()` computes the relative area of overlap for each grid cell polygon. It takes an xarray dataset and a geopandas dataframe as inputs.\n",
    "\n",
    "see the xagg documentation for more info [xagg.wrappers.pixel_overlaps](https://xagg.readthedocs.io/en/latest/xagg.html#xagg.wrappers.pixel_overlaps), [xagg.core.get_pixel_overlaps](https://xagg.readthedocs.io/en/latest/xagg.html#xagg.core.get_pixel_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmap = xagg.pixel_overlaps(t_nino_DJF_composite,countries,subset_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xagg.pixel_overlaps()` returns an object that contains \n",
    "1) a pandas dataframe with the grid cell polygon overlap information (access it with .agg), \n",
    "2) a dictionary containing the xarray data array source grid info (access it with .source_grid), \n",
    "3) a pandas series of geometry objects containing the geopandas source geometry info (access it with .geometry)\n",
    "4) if weights are input to pixel_overlaps, a pandas series of source grid weights, otherwise a string 'nowghts' (access it with .weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the dataframe with .agg\n",
    "weightmap.agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative area of a country polygon that each grid cell intersecting the country occupies is stored in the column `rel_area` and the corresponding centroid lat and lon of each grid cell is stored in the column `coords`.\n",
    "\n",
    "Here's how you would access the relative area of each pixel intersecting the first polygon/mulipolygon (Argentina) from the dataframe. This indexing returns a pandas series of 351 values. Meaning, there are 351 grid cells of our xarray data array that intersect the Argentina polygon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weightmap.agg['rel_area'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates (grid cell polygon centroids) that correspond to the `rel_area` values above would be indexed as follows. There are a total of 351 (lat,lon) tuples, which are returned as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weightmap.agg['coords'][0][0:4])\n",
    "\n",
    "print(len(weightmap.agg['coords'][0]),'total tuples in the list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each country we want to create a dataframe where each row contains data for a single grid cell. We want columns for lat, lon, relative area, data value, p value. We'll merge data on grid cell latitude and longitude, since those are the fields that all our data has in common. We'll test this merge of data on one country first using Boliva- index row 1 in the `weightmap.agg` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, convert t_nino_DJF_composite and t_nino_pval to pandas dataframes with columns for lat and lon\n",
    "t_nino_DJF_composite_df=t_nino_DJF_composite.to_dataframe().reset_index(level=[0,1])\n",
    "\n",
    "t_nino_pval.name='pval' # data array needs a name to be successfully converted to dataframe\n",
    "t_nino_pval_df=t_nino_pval.to_dataframe().reset_index(level=[0,1])\n",
    "\n",
    "t_nino_pval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now combine with the information from the weightmap for one country \n",
    "indexrow=1 # bolivia, index row 1\n",
    "\n",
    "# put coords (lat, lon) of grid cells overlapping this country into dataframe columns\n",
    "country_df=pd.DataFrame(weightmap.agg['coords'][indexrow],columns=['lat','lon'])\n",
    "\n",
    "# add rel_area as a dataframe column\n",
    "country_df['rel_area']=weightmap.agg['rel_area'][indexrow][0].reset_index(drop=True)\n",
    "\n",
    "# join the tavg and pval info\n",
    "country_df=country_df.merge(t_nino_DJF_composite_df, how='left',on=['lat','lon'])\n",
    "country_df=country_df.merge(t_nino_pval_df, how='left',on=['lat','lon'])\n",
    "country_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the merge subsets the 64800 rows in `t_nino_pval_df` and `t_nino_DJF_composite_df` to just the 124 that overlap with the Bolivia polygon. This is because we chose to merge 'left' (merge into `country_df`). Merge 'right' (merge into `t_nino_DJF_composite_df`, for example) would have kept all 64800 rows (global grid cells) and inserted nans in the `rel_area` outside of Bolivia.\n",
    "\n",
    "If we want to see which grid cells are ovelapping the Boliva polygon and how much each cell contributes to the polygon area, xagg has a function for that. Find more details in the xagg documentation [xagg.classes.weightmap](https://xagg.readthedocs.io/en/latest/xagg.html#xagg.classes.weightmap), [xagg.diag.diag_fig](https://xagg.readthedocs.io/en/latest/xagg.html#xagg.diag.diag_fig)\n",
    "\n",
    "Notice how the darkest grid cells are the northernmost cells that fall completely inside the polygon, while at the south end of the country grid cells completely inside the polygon are slightly less dark colored. This is because grid cells with evenly spaced lat, lon bounds are larger (more area) closer to the equator and smaller closer to the poles. So this is what we expect to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the relative grid cell weights in a figure\n",
    "weightmap.diag_fig(indexrow,t_nino_DJF_composite.to_dataset())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the relative area of Bolivia where nino winter temperature anomalies meet the 90% confidence level, we need to sum the `rel_area` column where the `pval` column is <= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_impacted=country_df.loc[country_df['pval']<=0.1].rel_area.sum()\n",
    "area_impacted=round(area_impacted*100.) # fraction to percent and limit precision\n",
    "\n",
    "print(area_impacted,'% of Bolivia experiences statistically significant temperature anomalies during strong winter el nino events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the result for all countries we can write a function and call it in a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_area_impacted(coords,rel_area,xr_pval):\n",
    "    df=pd.DataFrame(coords,columns=['lat','lon'])\n",
    "    df['rel_area']=rel_area.reset_index(drop=True)\n",
    "    \n",
    "    xr_pval.name='pval'\n",
    "    pval_df=xr_pval.to_dataframe().reset_index(level=[0,1])\n",
    "    df=df.merge(pval_df, how='left',on=['lat','lon'])\n",
    "\n",
    "    area_impacted=round(df.loc[df['pval']<=0.1].rel_area.sum()*100.)\n",
    "    return area_impacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through countries (rows of weightmap.agg dataframe)\n",
    "\n",
    "results={} # empty dictionary\n",
    "\n",
    "for index,row in weightmap.agg.iterrows():\n",
    "    answer=percent_area_impacted(row.coords,row.rel_area[0],t_nino_pval)\n",
    "    results[row.COUNTRY]=answer\n",
    "\n",
    "# create new dataframe with the results\n",
    "results_df=pd.DataFrame.from_dict(results,orient='index',columns=['T_PERCENT_AREA']).reset_index(names='COUNTRY')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the mean temperature anomaly per country using `xagg.core.aggregate()`. \n",
    "\n",
    "The result will be the area-weighted mean of `t_nino_DJF_composite` at all grid cells that overlap each country (including values that are and are not statistically significant).\n",
    "\n",
    "Here, the result appears in a column called `tavg` because the column name is based on the data variable name stored in the metadata of our xarray data array `t_nino_DJF_composite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_anomaly=xagg.core.aggregate(t_nino_DJF_composite,weightmap)\n",
    "mean_anomaly=mean_anomaly.to_dataframe()\n",
    "mean_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the dataframe to the relevant columns and adjust column names\n",
    "mean_anomaly=mean_anomaly[['COUNTRY','tavg']] # drop all but two columns\n",
    "mean_anomaly=mean_anomaly.rename(columns={'tavg':'MEAN_T_ANOMALY'})\n",
    "\n",
    "# merge into our results_df dataframe on the country name\n",
    "results_df=results_df.merge(mean_anomaly,how='left',on='COUNTRY')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the mean per country over only the grid cells with statistically significant anomalies is easy to do since xagg.pixel_overlaps() allows input of a set of weights. In our case, these weights will simply be a 0 and 1 mask where values of 1 represent t_nino_pval<=0.1. \n",
    "\n",
    "We need to create the mask, generate a new weightmap with the mask, and aggregate the data based on the new weight map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to input to xagg.pixel_overlaps\n",
    "mask = xr.where(t_nino_pval<=0.1,1,0) # 1 means statistically significant\n",
    "\n",
    "# generate new weightmap\n",
    "weightmap_mod = xagg.pixel_overlaps(t_nino_DJF_composite,countries,subset_bbox=False,weights=mask)\n",
    "\n",
    "# aggregate the t anomaly values only for grids with statistical significance\n",
    "# and export result to dataframe\n",
    "mean_anomaly_sigloc=xagg.core.aggregate(t_nino_DJF_composite,weightmap_mod)\n",
    "mean_anomaly_sigloc=mean_anomaly_sigloc.to_dataframe()\n",
    "\n",
    "# drop unneccesary columns\n",
    "mean_anomaly_sigloc=mean_anomaly_sigloc[['COUNTRY','tavg']]\n",
    "\n",
    "# rename the aggregated column\n",
    "mean_anomaly_sigloc=mean_anomaly_sigloc.rename(columns={'tavg':'MEAN_T_ANOMALY_SIGLOC'})\n",
    "\n",
    "# merge into our results dataframe\n",
    "results_df=results_df.merge(mean_anomaly_sigloc,how='left',on='COUNTRY')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side note about calculating polygon areas from gridded/raster data\n",
    "\n",
    "Here, our gridded data is nan over ocean grids (or grids that are mostly ocean). This means that a grid cell with a value of nan could slighly overlap where land is present. We have to take that into consideration when interpreting the accuracy/precision of these results.\n",
    "\n",
    "Take, for example, Trinidad and Tobago. The area we calculated is 98% instead of 100% because of the way the grid cells align with the coastline. This could be the case for any country with a coast.\n",
    "\n",
    "Let's take a closer look using xagg's diagnostic figure to look at which grid cells overlap Trinidad and Tobago (row 11 of weightmap.agg dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexrow=11\n",
    "weightmap.diag_fig(indexrow,t_nino_DJF_composite.to_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now make a plot to look at which grid cells of our data are considered ocean (nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure with coastlines\n",
    "fig=plt.figure(figsize=(6,6))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "\n",
    "# color nan grey\n",
    "cmap = plt.colormaps.get_cmap(\"RdBu_r\").copy()\n",
    "cmap.set_bad('grey') \n",
    "\n",
    "# subset based on the above plot's lat and lon\n",
    "latmin,latmax = 9.5, 11.5\n",
    "lonmin,lonmax = -62,-60\n",
    "t_nino_DJF_composite.sel(lat=slice(latmin,latmax),lon=slice(lonmin,lonmax)).plot(cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, a small part of the island is covered by an ocean grid where the data value is nan (colored grey). This is why our T_PERCENT_AREA is 98%. The nan grid cell accounts for the other 2% area of the country. It's just something to be aware of and to take into consideration when choosing a precision for presenting your results. It wouldn't make sense to present the results in this case to multiple decimal places because of this limitation of our data/analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnbydoing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
