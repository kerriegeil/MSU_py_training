{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Learn by Doing: ENSO Analysis\n",
    "\n",
    "**Developed By:** Dr. Kerrie Geil, Mississippi State University\n",
    "\n",
    "**Date:** May 2024\n",
    "\n",
    "**Package Requirements:** xarray, netcdf4, numpy, pandas, scipy, rioxarray, matplotlib, cartopy, jupyter\n",
    "\n",
    "**Links:** **[github repo link](https://github.com/kerriegeil/MSU_py_training)**, [link to this notebook](https://github.com/kerriegeil/MSU_py_training/blob/main/learn_by_doing/enso_analysis.ipynb)\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook helps the learner build intermediate python programming skills through data query, manipulation, analysis, and visualization. Learning will be centered around the El Nino Southern Oscillation (ENSO) mode of global climate variability and its relationship to global temperature and precipitation. The notebook is aimed at learners who already have intermediate programming skills and some knowledge of statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection of Useful Links\n",
    "\n",
    "- [documentation page for every version of Python](https://www.python.org/doc/versions/)\n",
    "- every version of Python also includes a tutorial e.g. [The Python Tutorial v3.12.3](https://docs.python.org/release/3.12.3/tutorial/index.html)\n",
    "- this is not technically a link, but Google is your friend! If there is something you want to learn how to do with python, just Google it and click through the search results to see if you can find an answer. A Google search will often return results from online forums like Stack Overflow where someone has asked the same question and received several answers. This same principle works for figuring out conda and jupyter things. Chances are the answer is a quick web search away\n",
    "- [jupyter markdown cheat sheet](https://notebook.community/tschinz/iPython_Workspace/00_Admin/CheatSheet/Markdown%20CheatSheet)\n",
    "- enso background link\n",
    "- [xarray documentation](https://docs.xarray.dev/en/stable/) which includes the api reference, getting started guide, user guide, and developer info\n",
    "- [kerrie's github repo](https://github.com/kerriegeil/MSU_py_training) is the current location where this notebooks lives and receives updates, this may eventually move to an MSU enterprise repo\n",
    "- **[PythonWorkshop_Data.zip](https://www.northerngulfinstitute.org/projects/rgmg/PythonWorkshop_Data.zip)**, a temporary link to the data used in the python: learn by doing workshop, we're still working on a permanent solution for hosting the data and notebooks but data should be available at this link until 3 June 2024\n",
    "- [Statistical methods in the atmospheric sciences by Daniel Wilks](https://www.google.com/books/edition/Statistical_Methods_in_the_Atmospheric_S/fxPiH9Ef9VoC?hl=en&gbpv=0) the textbook I turn to often when I need a climate science statistics refresher (very descriptive with examples)\n",
    "- [Statistical analysis in climate research by von Storch and Zwiers](https://www.google.com/books/edition/Statistical_Analysis_in_Climate_Research/bs8hAwAAQBAJ?hl=en&gbpv=0) the quick reference I turn to for climate science statistics (less descriptive, but broader content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the El Nino Southern Oscillation (ENSO)\n",
    "\n",
    "\n",
    "\n",
    "<video controls width=\"560\" height=\"315\" src=\"https://oceanservice.noaa.gov/facts/elninolanina/otkn_721_elninolanina_lg.mp4\">animation</video>\n",
    "\n",
    "\n",
    "ENSO is a feature of Earth's natural climate variability that impacts weather globally. It has 3 phases (described in the short video above): El Nino (warm tropical Pacific), La Nina (cool tropical Pacific), and neutral. \n",
    "\n",
    "Over North America, the phase of ENSO modulates the location of the jet stream as well as the storm systems traveling along the jet stream. This results in areas of wetter/drier and cooler/warmer weather during El Nino and La Nina conditions. Weather impacts from El Nino and La Nina are generally strongest in winter months over North America.\n",
    "\n",
    "Here's a bit more reading if you are interested:\n",
    "- [NOAA Ocean Service: what are el nino and la nina](https://oceanservice.noaa.gov/facts/ninonina.html#:~:text=El%20Ni%C3%B1o%20causes%20the%20Pacific,life%20off%20the%20Pacific%20coast.)\n",
    "- [NOAA climate.gov: ENSO in a nutshell](https://www.climate.gov/news-features/blogs/enso/what-el-ni%C3%B1o%E2%80%93southern-oscillation-enso-nutshell)\n",
    "- [NOAA: the jet stream](https://www.noaa.gov/jetstream/global/jet-stream) and [NOAA: what is the jet stream?](https://scijinks.gov/jet-stream/#:~:text=Jet%20streams%20are%20located%20about,where%20we%20live%20and%20breathe.)\n",
    "\n",
    "Scientists have developed a few different \"indexes\" for tracking the phase and strength of ENSO. These indexes are usually comprised of monthly sea surface temperature anomalies from different regions in the tropical Pacific ocean. However, there are many ENSO indexes including some that are created from surface pressure anomalies and others that are multivariate. Here's some more information.\n",
    "- [UCAR Climate Data Guide: ENSO SST indices](https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni)\n",
    "- [NOAA PSL: ENSO Index Dashboard](https://psl.noaa.gov/enso/dashboard.html)\n",
    "- [NOAA climate.gov: why are there so many ENSO indexes?](https://www.climate.gov/news-features/blogs/enso/why-are-there-so-many-enso-indexes-instead-just-one)\n",
    "\n",
    "In this notebook, we'll ask 6 science questions about ENSO and answer them using an ENSO index (Nino3.4 SST Index) as well as global sea surface temperature, air temperature, and preciptation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Questions\n",
    "\n",
    "We'll investigate the following ENSO-related science questions (using simple statistics) as a way to pick up some useful intermediate python programming skills:\n",
    "\n",
    "1) How many strong El Nino and La Nina events occurred from 1948-2023? \n",
    "2) Using composite analysis, what pattern do we see in sea surface temperature during strong El Nino and La Nina conditions? What is the approximate magnitude of the equatorial Pacific SST anomaly during these conditions?\n",
    "3) Using composite analysis, where do strong El Nino and La Nina conditions during boreal winter (DJF) have a statistically significant relationship with winter temperature and precipitation globally?\n",
    "4) Do the spatial patterns of anomalous temperature and precipitation during strong EL Nino and strong La Nina look similar? What is similar and different?\n",
    "5) Where, globally, is there a statistically significant correlation between the winter mean (DJF) nino3.4 index and winter mean temperature and precipitation? \n",
    "6) How much variance in winter mean (DJF) precipitation and temperature is explained (r squared) by the winter mean nino3.4 index at the following locations? Pilot Station, AK; El Paso, TX; Medellin, Colombia; Johannesburg, South Africa; Davao, Philippines; Astana, Kazakhstan\n",
    "\n",
    "**Disclaimer:** This notebook is intended for python programming learning. There are many datasets and statistical methods we could use to answer our science questions. The techniques used in this notebook are chosen for their simplicity since we are focused on learning intermediate programming skills as opposed to a focus on producing peer-review level analyses. You will undoubtedly see different techniques, thresholds, seasons, and more complex statisical methods used in ENSO literature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python Packages and Defining Your Workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from scipy.signal import detrend\n",
    "import rioxarray as rio\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from cartopy.mpl.gridliner import LongitudeFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'data/ENSO/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Data\n",
    "\n",
    "We will need the following data over many data years:\n",
    "\n",
    "\n",
    "data description | frequency | units | dataset name | source\n",
    "---|---|---|---|---\n",
    "nino 3.4 sea surface temperature index | monthly | C | Nino 3.4 SST index | [NOAA PSL](https://psl.noaa.gov/gcos_wgsp/Timeseries/Nino34/)\n",
    "sea surface temperature | monthly | C | JMA COBE2 | [via NOAA PSL](https://psl.noaa.gov/data/gridded/data.cobe2.html)\n",
    "average air temperature | monthly | C | BEST | [Berkeley Earth](https://berkeleyearth.org/data/)\n",
    "precipitation | monthly | mm/day | NOAA PREC/L | [NOAA PSL](https://psl.noaa.gov/data/gridded/data.precl.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color=\"green\"><b>**If you haven't already, download **[PythonWorkshop_Data.zip](https://www.northerngulfinstitute.org/projects/rgmg/PythonWorkshop_Data.zip)** and unzip to the same directory where you have this notebook saved.**</b></font> (After unzipping you should have the directory data/ at the same level in your file system as this notebook).\n",
    "\n",
    "\n",
    "Scripted downloads of the individual datasets can be found in a separate notebook called [get_enso_datasets.ipynb](https://github.com/kerriegeil/MSU_py_training/blob/main/learn_by_doing/get_datasets/get_enso_datasets.ipynb). \n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "We'll execute the following steps on each dataset to prepare our data for analysis:\n",
    "\n",
    "- make sure every dataset has identical dimension names and dimension order (time,lat,lon)\n",
    "- make sure every dataset has identical time coordinate labels (datetimes at month start)\n",
    "- subset to our analysis years 1948-2023 (the years that all datasets have in common)\n",
    "- make sure longitudes are ascending from -180 to 180\n",
    "- make sure latitudes are ascending\n",
    "- calculate anomalies using base period 1981-2010 for sea surface temperature, precipitation, and air temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames\n",
    "nino_f = data_dir+'nino34_anomalies_monthly_NOAA.txt'\n",
    "sst_f = data_dir+'sst_monthly_COBE2_JMA.nc'\n",
    "t_f = data_dir+'tavg_monthly_BerkeleyEarth.nc'\n",
    "pr_f = data_dir+'precip_monthly_PRECL_NOAA.nc'\n",
    "\n",
    "# data years in common across all the datasets is 1948-2023\n",
    "# we'll subset in time to these years\n",
    "year_start = '1948'\n",
    "year_end = '2023'\n",
    "\n",
    "# base period years (for anomalies)\n",
    "base_start = '1981'\n",
    "base_end = '2010'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nino 3.4 Index\n",
    "\n",
    "This csv data file contains a row for each year of data and each column is one of 12 monthly anomaly values for the Nino 3.4 area. The base period used is 1981-2010.\n",
    "\n",
    "There are plenty of ways to load data from a csv file, we'll use **[pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)** because there's some extra info in our data file beyond just rows and columns of numbers, and this function has a lot of useful parameters we can use.\n",
    "\n",
    "Ultimately, we'll want to move this data into an xarray DataArray structure to make our lives easier when working with our other data variables that come from netcdf files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nino3.4 index data into a pandas dataframe\n",
    "\n",
    "nino_raw=pd.read_csv(nino_f,sep='\\s+',skiprows=1,skipfooter=7,header=None,index_col=0,na_values=-99.99,engine='python')\n",
    "nino_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to go from pandas dataframes to xarray data arrays. Here, we'll first convert the 2D pandas dataframe (rows,columns) into a 1 dimensional numpy array (one long list of data values that will be ordered in time from 1870-01 to 2024-12). \n",
    "\n",
    "In the next line of code `.to_numpy()` comes from the pandas library **[pandas.DataFrame.to_numpy()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html)** and `.flatten()` comes from the numpy library **[numpy.ndarray.flatten()](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse the data into a 1D array (continuous timeseries)\n",
    "# pandas dataframe --> 1D numpy array\n",
    "nino=nino_raw.to_numpy().flatten()\n",
    "\n",
    "len(nino),nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create monthly datetimes\n",
    "dates=pd.date_range('1870-01-01','2024-12-01',freq='MS')\n",
    "\n",
    "len(dates),dates[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an xarray object with metadata labels (time dimension and coordinate)\n",
    "# 1D numpy array --> 1D xarray data array\n",
    "nino=xr.DataArray(nino,name='nino',dims='time',coords={'time':dates})\n",
    "\n",
    "# assign some variable attributes\n",
    "nino.attrs['standard_name']='nino3.4 index'\n",
    "nino.attrs['units']='C'\n",
    "nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset in time using labels\n",
    "nino=nino.sel(time=slice(year_start,year_end))\n",
    "nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "nino.plot()\n",
    "plt.axhline(y=0,color='grey',linestyle='dashed',linewidth=0.5)\n",
    "plt.title(\"Nino 3.4 Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Surface Temperature (SST)\n",
    "\n",
    "Is 3-dimensional data provided in a netcdf file, so we'll open it with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "ds=xr.open_dataset(sst_f)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull variable from xarray dataset\n",
    "sst=ds.sst\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder latitudes ascending\n",
    "sst=sst.reindex(lat=sst.lat[::-1])\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset in time\n",
    "# sst's time labels already match nino's so we don't need to re-assign time labels, just subset\n",
    "sst=sst.sel(time=slice(year_start,year_end))\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change longitudes from 0 to 360 to -180 to 180\n",
    "# some plotting functions have problems when longitude is 0 to 360\n",
    "\n",
    "sst.coords['lon']=xr.where(sst.coords['lon']>180,sst.coords['lon']-360,sst.coords['lon'])\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder longitudes so they are ascending\n",
    "# we'll use roll, sort would also work\n",
    "roll_len=len(sst.lon)//2\n",
    "sst=sst.roll(lon=roll_len,roll_coords=True)\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies\n",
    "\n",
    "# first calculate the monthly climatological values (long term means) over the base period\n",
    "sst_base=sst.sel(time=slice(base_start,base_end))  # subset in time\n",
    "sst_clim=sst_base.groupby(sst_base.time.dt.month).mean('time')  # long term mean of each month\n",
    "sst_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the anomalies\n",
    "sst_anom=sst.groupby(sst.time.dt.month) - sst_clim\n",
    "\n",
    "# assign some variable attributes\n",
    "sst_anom.attrs['standard_name']='sst anomaly'\n",
    "sst_anom.attrs['units']='C'\n",
    "sst_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check the anomalies for a single month\n",
    "ptime='2020-01'\n",
    "\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.xaxis.set_visible(True)\n",
    "ax.yaxis.set_visible(True)\n",
    "\n",
    "sst_anom.sel(time=ptime).plot(ax=ax)\n",
    "plt.title('SST anomalies on '+ptime)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precipitation\n",
    "\n",
    "Is 3-dimensional data provided in a netcdf file, so we'll open it with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "ds=xr.open_dataset(pr_f)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull variable from xr dataset\n",
    "pr=ds.precip\n",
    "\n",
    "# this data's time labels already match nino's so we don't need to re-assign the coordinate labels\n",
    "# just subset\n",
    "pr=pr.sel(time=slice(year_start,year_end))\n",
    "\n",
    "# latitudes ascending\n",
    "pr=pr.reindex(lat=pr.lat[::-1])\n",
    "\n",
    "# change longitudes from 0 to 360 to -180 to 180\n",
    "pr.coords['lon']=xr.where(pr.coords['lon']>180,pr.coords['lon']-360,pr.coords['lon'])\n",
    "roll_len=len(pr.lon)//2\n",
    "pr=pr.roll(lon=roll_len,roll_coords=True)\n",
    "\n",
    "# calculate anomalies\n",
    "pr_base=pr.sel(time=slice(base_start,base_end)) # subset to base period\n",
    "pr_clim=pr_base.groupby(pr_base.time.dt.month).mean('time') # monthly climatological values\n",
    "pr_anom=pr.groupby(pr.time.dt.month) - pr_clim  # anomalies\n",
    "\n",
    "# assign some variables attributes\n",
    "pr_anom.attrs['standard_name']='pr anomaly'\n",
    "pr_anom.attrs['units']='mm/day'\n",
    "\n",
    "pr_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check the anomalies for a single month\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.xaxis.set_visible(True)\n",
    "ax.yaxis.set_visible(True)\n",
    "\n",
    "pr_anom.sel(time=ptime).plot(vmin=-20,vmax=20,extend='both',cmap='BrBG',cbar_kwargs={'shrink':0.9})\n",
    "plt.title('PR anomalies '+ptime)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Is 3-dimensional data provided in a netcdf file, so we'll open it with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "ds=xr.open_dataset(t_f)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of things that are different about this data file compared to the others\n",
    "- it is provided as anomalies using base period 1951-1980\n",
    "- it has multiple data variables in the file\n",
    "- it has an extra dimension due to the climatology variable included in the file\n",
    "- the latitude and longitude dimension names (vs lat and lon)\n",
    "- the longitudes are -180 to 180 already\n",
    "- the latitudes are sorted ascending already\n",
    "- the time coordinate is given as float values instead of datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace whacky float times with datetimes to match the other datasets\n",
    "dates=pd.date_range('1750-01-01','2024-03-01',freq='MS')\n",
    "ds['time']=dates\n",
    "\n",
    "# we also need to rename the dimension 'month_number' for groupby to work correctly (groupby likes the label 'month')\n",
    "# and so we don't trip up later we'll rename latitude longitude to lat and lon like the other datasets\n",
    "ds=ds.rename({'month_number':'month','latitude':'lat','longitude':'lon'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this data is provided as anomalies using the base period 1951-1980, we'll change the base period to match the rest of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change base period\n",
    "\n",
    "# pull variables from xr dataset\n",
    "t_anom_5180=ds.temperature\n",
    "clim_5180=ds.climatology\n",
    "\n",
    "# create temperature values: anomalies plus climatology\n",
    "t=t_anom_5180.groupby(t_anom_5180.time.dt.month)+clim_5180\n",
    "\n",
    "# calculate climatological values on new base period\n",
    "t_base=t.sel(time=slice(base_start,base_end))  # subset in time\n",
    "clim_8110 = t_base.groupby(t_base.time.dt.month).mean('time')  # long term means for each month\n",
    "\n",
    "# calculate anomalies with new base period\n",
    "t_anom=t.groupby(t.time.dt.month)-clim_8110\n",
    "\n",
    "# subset in time to match other datasets\n",
    "t_anom=t_anom.sel(time=slice(year_start,year_end))\n",
    "\n",
    "# assign some variable attributes\n",
    "t_anom=t_anom.rename('tavg')\n",
    "t_anom.attrs['standard_name']='T anomaly'\n",
    "t_anom.attrs['units']='C'\n",
    "\n",
    "t_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check the anomalies for a single month\n",
    "\n",
    "fig=plt.figure(figsize=(10,5))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.xaxis.set_visible(True)\n",
    "ax.yaxis.set_visible(True)\n",
    "\n",
    "t_anom.sel(time=ptime).plot()\n",
    "plt.title('T anomalies '+ptime)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start our analysis with the variables `nino`, `sst_anom`, `pr_anom`, and `t_anom`\n",
    "\n",
    "Let's double check our 4 variable shapes below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data shapes\n",
    "nino.shape, sst_anom.shape, pr_anom.shape, t_anom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first and last time is the same for all data\n",
    "# another way to do this would be with assert statements or with numpy.testing\n",
    "\n",
    "variables=[nino, sst_anom, pr_anom, t_anom] # list of arrays\n",
    "\n",
    "for var in variables:\n",
    "    print(var.name, var.time[0].data,var.time[-1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up \n",
    "del ds, nino_raw, pr, pr_base, pr_clim, sst, sst_base, sst_clim, t, t_anom_5180, t_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Main Analysis\n",
    "\n",
    "## 1) How many strong El Nino and La Nina events have occurred from 1948 to 2023? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to this question, of course, depends on how we define strong ENSO events.\n",
    "\n",
    "There are multiple methods for identifying ENSO events. We will use the following criteria to identify strong events:\n",
    "- Input data: Nino 3.4 Index 5-month centered running mean\n",
    "- Criteria: 5 consecutive months exceeding the threshold value\n",
    "- Threshold: +/- 0.6 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants based on our criteria\n",
    "nmonths=5\n",
    "event_thresh=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first calculate the 5-month rolling mean\n",
    "nino_rollmean=nino.rolling(time=nmonths,center=True).mean()\n",
    "\n",
    "# plot it\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y = -event_thresh, color='purple', linestyle='dashed', linewidth=0.5) # nina threshold\n",
    "plt.axhline(y = 0,color='grey', linestyle='dashed', linewidth=0.5)  # zero reference line\n",
    "plt.axhline(y = event_thresh, color='purple', linestyle='dashed', linewidth=0.5)  # nino threshold\n",
    "\n",
    "nino_rollmean.plot()\n",
    "plt.title(\"Nino 3.4 Index 5-month Rolling Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anywhere the nino3.4 rolling mean (blue line) exceeds the thresholds (purple lines) is potentially an ENSO event. To identify which peaks and valleys in the timeseries qualify as ENSO events we need to identify where the thresholds are exceeded for at least 5 consecutive months.  \n",
    "\n",
    "We'll use a for loop to identify ENSO events in the timeseries and mark months \n",
    "- during an El Nino event with +1\n",
    "- during a La Nina event with -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array to hold our results and initialize to nan\n",
    "# this array is where we will fill values with +1,-1\n",
    "nino_events=nino_rollmean.copy() \n",
    "nino_events[:]=np.nan\n",
    "\n",
    "# look at the first few values, should be all nan\n",
    "nino_events[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now loop through months and fill +1, -1 for windows of 5 months that meet our criteria\n",
    "\n",
    "for i,value in enumerate(nino_rollmean):\n",
    "    # La Nina conditions\n",
    "    if  value < -event_thresh:\n",
    "        # possible La Nina conditions, look forward 4 more months\n",
    "        window=nino_rollmean[i:i+nmonths]\n",
    "        if all(window < -event_thresh):\n",
    "            nino_events[i:i+nmonths] = -1\n",
    "\n",
    "    # El Nino conditions\n",
    "    if  value > event_thresh:\n",
    "        # possible El Nino conditions, look forward 4 more months\n",
    "        window=nino_rollmean[i:i+nmonths]\n",
    "        if all(window > event_thresh):\n",
    "            nino_events[i:i+nmonths]=1     \n",
    "            \n",
    "# plot it\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.axhline(y = 0, color='grey', linestyle='dashed', linewidth=0.5)  # zero reference line\n",
    "nino_events.plot(linestyle='None', marker='o', markersize=1)\n",
    "plt.title(\"strong ENSO events (El Nino = 1, La Nina = -1)\")\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have identified all the periods that meet our criteria, but this plot isn't too nice to look at.\n",
    "\n",
    "Let's use the `nino_events` array to add shading to the nino_rollmean plot as well as count how many el nino and la nina events there are in the timeseries. To do this we'll need to find where each ENSO event starts and ends. For this we've written a custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_bounds(event_array,event_id):\n",
    "    '''\n",
    "    this function accepts a 1D xr data array of categorical data values with dim 'time' of datetimes and returns the \n",
    "    beginning and ending datetimes for each span of consecutive occurrences for a single categorical value  \n",
    "    \n",
    "    Parameters:\n",
    "    event_array: the 1D xr data array of categorical data values with dim 'time' of datetimes\n",
    "    event_id: the categorical data value to use\n",
    "    \n",
    "    Returns:\n",
    "    bounds: a list of datetime tuples [(starting dateime, ending datetime), ...]\n",
    "    '''\n",
    "    \n",
    "    bounds=[] # empty list to hold the results\n",
    "    start_flag=False # flag for identifying the start of an event\n",
    "    istart=0  # index of the event start, initialized to zero\n",
    "    iend=0    # index of the event end, initialized to zero\n",
    "    \n",
    "    for i,val in enumerate(event_array[:-1]):    \n",
    "        # find each event start\n",
    "        if (val==event_id) and (start_flag==False):\n",
    "            istart=i\n",
    "            start_flag=True\n",
    "        # find each event end, save start/end times to a list, reset istart iend to zero\n",
    "        if (start_flag) and (iend==0) and (event_array[i+1]!=event_id):\n",
    "            iend=i\n",
    "            # append a tuple (event start time, event end time) to our list of results\n",
    "            start_time=event_array.time[istart].dt.strftime('%Y-%m-%d')\n",
    "            end_time=event_array.time[iend].dt.strftime('%Y-%m-%d')\n",
    "            bounds.append((start_time.data,end_time.data))\n",
    "            # reset values so we can look for the next event\n",
    "            start_flag=False\n",
    "            iend=0\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our function\n",
    "nino_bounds=get_event_bounds(nino_events,1)\n",
    "nina_bounds=get_event_bounds(nino_events,-1)\n",
    "\n",
    "# look at the first few results in the list\n",
    "for tup in nino_bounds[0:3]:\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at two ways to iterate through our list of tuples\n",
    "\n",
    "# the loop variable \"tup\" is each item (a tuple) from the list \n",
    "for tup in nino_bounds[0:3]:\n",
    "    print(tup, tup[0], tup[1])\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "# the loop variables \"val1\" \"val2\" are the values inside each item (tuple) in the list\n",
    "for val1,val2 in nino_bounds[0:3]:\n",
    "    print(val1,val2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 lists `nino_bounds` and `nina_bounds` containing tuples of the start and end datetime for each event. \n",
    "\n",
    "The length of each list will tell us how many el nino and la nina events we found in the time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('How many strong el nino and la nina events occurred from 1948 to 2023?')\n",
    "print(len(nino_bounds),'strong el nino events')\n",
    "print(len(nina_bounds),'strong la nina events') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Nino 3.4 rolling mean with shading during strong el nino and la nina conditions\n",
    "\n",
    "fig=plt.figure(figsize=(15,2))\n",
    "\n",
    "# horizontal guide lines\n",
    "plt.axhline(y=-event_thresh,color='purple',linestyle='dashed',linewidth=0.5) # nina threshold\n",
    "plt.axhline(y=0,color='grey',linestyle='dashed',linewidth=0.5)  # zero guideline\n",
    "plt.axhline(y=event_thresh,color='purple',linestyle='dashed',linewidth=0.5)  # nino threshold\n",
    "\n",
    "# plot the rolling mean timeseries with title\n",
    "nino_rollmean.plot()\n",
    "plt.title(\"Nino 3.4 Index 5-mo Rolling Mean with shading for nino/nina/neutral conditions\")\n",
    "\n",
    "# add blue shading during nino events\n",
    "for tstart,tend in nino_bounds:\n",
    "    plt.axvspan(tstart, tend, color='cyan', alpha=0.25, lw=0)\n",
    "\n",
    "# add yellow shading during nina events    \n",
    "for tstart,tend in nina_bounds:\n",
    "    plt.axvspan(tstart, tend, color='gold', alpha=0.25, lw=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Using composite analysis, what pattern do we see in sea surface temperature during El Nino and La Nina conditions? What is the approximate magnitude of the equatorial Pacific SST anomaly during these conditions?\n",
    "\n",
    "Remember, our array of sea surface temperature anomalies is called `sst_anom` and we've identified strong el nino and la nina periods in the array called `nino_events`\n",
    "\n",
    "Our composites will be the time-mean of a group of SST anomaly maps for different months. \n",
    "\n",
    "We'll make a composite of SST anomalies for months with el nino conditions and another composite of SST anomalies for months with la nina conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el nino composite\n",
    "\n",
    "# keep sst anomalies only for months during el nino events, then average in time\n",
    "sst_nino_composite=sst_anom.where(nino_events==1).mean('time',keep_attrs=True)\n",
    "\n",
    "# plot the composite, centered over the Pacific\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree(central_longitude=180))\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.xaxis.set_visible(True)  # show lons\n",
    "ax.yaxis.set_visible(True)  # show lats\n",
    "\n",
    "# correct tick labels for lon\n",
    "lon_formatter = LongitudeFormatter(direction_label=False)\n",
    "ax.xaxis.set_major_formatter(lon_formatter) \n",
    "\n",
    "sst_nino_composite.plot(cmap='RdBu_r',transform=ccrs.PlateCarree())\n",
    "plt.title('composite of SST anomalies during strong el nino events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la nina composite\n",
    "\n",
    "# keep sst anomalies only for months during la nina events, then average in time\n",
    "sst_nina_composite=sst_anom.where(nino_events==-1).mean('time',keep_attrs=True)\n",
    "\n",
    "# plot the composite, centered over the Pacific\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree(central_longitude=180))\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.xaxis.set_visible(True) # show lons\n",
    "ax.yaxis.set_visible(True) # show lats\n",
    "\n",
    "# correct tick labels for lon\n",
    "lon_formatter = LongitudeFormatter(direction_label=False)\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "\n",
    "sst_nina_composite.plot(cmap='RdBu_r',transform=ccrs.PlateCarree())\n",
    "plt.title('composite of SST anomalies during strong la nina events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the approximate magnitude of the el nino and la nina SST anomalies in the equatorial pacific, we can just eyeball the colorbar on the composite maps. Or, do a quick min max on a selection of latitudes and longitudes in the tropical pacific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latS = -3\n",
    "latN = 3\n",
    "lonW = -170\n",
    "lonE = -100\n",
    "\n",
    "nino_range = ( round(sst_nino_composite.sel(lat=slice(latS,latN),lon=slice(lonW,lonE)).min().item(), 1),\n",
    "               round(sst_nino_composite.sel(lat=slice(latS,latN),lon=slice(lonW,lonE)).max().item(), 1) )\n",
    "\n",
    "nina_range = ( round(sst_nina_composite.sel(lat=slice(latS,latN),lon=slice(lonW,lonE)).max().item(), 1),\n",
    "                round(sst_nina_composite.sel(lat=slice(latS,latN),lon=slice(lonW,lonE)).min().item(), 1) )\n",
    "\n",
    "print(f'the equatorial pacific anomalies in the el nino composite range from about {nino_range}')\n",
    "print(f'the equatorial pacific anomalies in the la nina composite range from about {nina_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Using composite analysis, where do strong El Nino and La Nina conditions during boreal winter (DJF) have a statistically significant relationship with winter temperature and precipitation globally?\n",
    "\n",
    "We'll use our `nino_events` array to select months from our temperature and precipitation data `t_anom` and `pr_anom` for making composites. \n",
    "\n",
    "To determine statistical significance, we'll use a t-test for difference in means between two data samples. For temperature and precipitation separately, we'll test significance between the following data samples:\n",
    "- **sample 1:** winter months during strong el nino conditions, **sample 2:** all other winter months\n",
    "- **sample 1:** winter months during strong la nina conditions, **sample 2:** all other winter months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with el nino conditions, temperature\n",
    "# get temperature anomalies only for times during strong el nino conditions\n",
    "\n",
    "t_nino=t_anom.where(nino_events==1,drop=True)\n",
    "t_nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now separate out winter DJF months\n",
    "# this is sample 1: winter months during strong el nino conditions\n",
    "\n",
    "t_nino_DJF=t_nino.groupby(t_nino.time.dt.season)['DJF'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a composite\n",
    "t_nino_DJF_composite=t_nino_DJF.mean('time',keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for precipitation\n",
    "pr_nino=pr_anom.where(nino_events==1,drop=True) # all nino precip\n",
    "pr_nino_DJF=pr_nino.groupby(pr_nino.time.dt.season)['DJF'] # sample 1: winter months during strong el nino conditions\n",
    "pr_nino_DJF_composite=pr_nino_DJF.mean('time',keep_attrs=True) # make composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot el nino composites, don't worry about statistical significance yet\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "t_nino_DJF_composite.plot(cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title('winter mean temperature anomalies\\n during strong El Nino conditions')\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "pr_nino_DJF_composite.plot(vmin=-2, vmax=2, extend='both',cmap='BrBG',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title('winter mean precipitation anomalies\\n during strong El Nino conditions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where are the above anomalies statistically significant?**   \n",
    "\n",
    "We need to create sample 2 for temperature and precipitation, then apply our test statistic comparing sample 1 to sample2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a t and pr samples that include all winter months DJF when there are not strong el nino conditions\n",
    "\n",
    "# all months that don't fall in strong nino events\n",
    "t_other=t_anom.where(nino_events!=1,drop=True) \n",
    "\n",
    "# pull out just DJF months\n",
    "t_other_DJF=t_other.groupby(t_other.time.dt.season)['DJF'] # this is sample 2: all winter months that are NOT during strong el nino conditions\n",
    "\n",
    "# same for precip\n",
    "pr_other=pr_anom.where(nino_events!=1,drop=True)\n",
    "pr_other_DJF=pr_other.groupby(pr_other.time.dt.season)['DJF']  # this is sample 2: all winter months that are NOT during strong el nino conditions\n",
    "\n",
    "# look at how many months of data are included in each sample\n",
    "print('t nino and non-nino sample sizes:',t_nino_DJF.shape[0],t_other_DJF.shape[0]) \n",
    "print('pr nino and non-nino data sample sizes:',pr_nino_DJF.shape[0],pr_other_DJF.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy has a function that will calculate the t-test for difference in means between two data samples **[scipy.stats.ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)**\n",
    "\n",
    "This function can accept xarray data array inputs. We'll test sample 1 = `t_nino_DJF` vs sample 2 = `t_other_DJF` as well as sample 1 = `pr_nino_DJF` vs sample 2 = `pr_other_DJF`. \n",
    "\n",
    "Notice that scipy.stats.ttest_ind will return an object with numpy arrays attached. You can access the returned data like `result.pvalue`, `result.statistic`, `result.df`. \n",
    "\n",
    "For easy of plotting it's a good idea to convert the numpy result back to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test for difference in means \n",
    "t_sigtest = ss.ttest_ind(t_nino_DJF, t_other_DJF, axis=0, equal_var=False)\n",
    "\n",
    "# numpy --> xarray\n",
    "t_nino_pval = xr.DataArray(t_sigtest.pvalue, coords={'lat':('lat',t_nino.coords['lat'].data),'lon':('lon',t_nino.coords['lon'].data)})  \n",
    "\n",
    "# same thing for pr\n",
    "pr_sigtest=ss.ttest_ind(pr_nino_DJF,pr_other_DJF,axis=0,equal_var=False)\n",
    "pr_nino_pval=xr.DataArray(pr_sigtest.pvalue, coords={'lat':('lat',pr_nino.coords['lat'].data),'lon':('lon',pr_nino.coords['lon'].data)})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the same plot as above but only show the results where pval < 0.1\n",
    "pval=0.1\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "t_nino_DJF_composite.where(t_nino_pval<pval).plot(cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean temperature anomalies\\n during strong El Nino conditions (p < {pval})')\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "pr_nino_DJF_composite.where(pr_nino_pval<pval).plot(vmin=-2, vmax=2, extend='both',cmap='BrBG',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean precipitation anomalies\\n during strong El Nino conditions (p < {pval})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**repeat the above for la nina conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la nina\n",
    "\n",
    "# create sample 1's and composites\n",
    "# temperature\n",
    "t_nina = t_anom.where(nino_events == -1,drop=True) # t during strong la nina\n",
    "t_nina_DJF = t_nina.groupby(t_nina.time.dt.season)['DJF']  # sample 1: winter months during strong nina\n",
    "t_nina_DJF_composite = t_nina_DJF.mean('time',keep_attrs=True) # make composite\n",
    "# precipitation\n",
    "pr_nina = pr_anom.where(nino_events == -1,drop=True) # pr during strong la nina\n",
    "pr_nina_DJF = pr_nina.groupby(pr_nina.time.dt.season)['DJF'] # sample 1: winter months during strong nina\n",
    "pr_nina_DJF_composite = pr_nina_DJF.mean('time',keep_attrs=True) # make composite\n",
    "\n",
    "# create sample 2's\n",
    "t_other = t_anom.where(nino_events != -1,drop=True) # all months that are NOT strong nina\n",
    "t_other_DJF = t_other.groupby(t_other.time.dt.season)['DJF'] # sample 2: all winter months that are NOT during strong nina\n",
    "pr_other = pr_anom.where(nino_events != -1,drop=True) # all months that are NOT strong nina\n",
    "pr_other_DJF = pr_other.groupby(pr_other.time.dt.season)['DJF']  # sample 2: all winter months that are NOT during strong nina\n",
    "\n",
    "# look at how many months of data are included in each sample\n",
    "print('t nina and non-nina sample sizes:',t_nina_DJF.shape[0],t_other_DJF.shape[0]) \n",
    "print('pr nina and non-nina data sample sizes:',pr_nina_DJF.shape[0],pr_other_DJF.shape[0]) \n",
    "\n",
    "# statistical significance\n",
    "t_sigtest = ss.ttest_ind(t_nina_DJF, t_other_DJF, axis=0, equal_var=False) # t-test for difference in means \n",
    "t_nina_pval = xr.DataArray(t_sigtest.pvalue, \n",
    "                      coords={'lat':('lat',t_nino.coords['lat'].data),\n",
    "                              'lon':('lon',t_nino.coords['lon'].data)})  # numpy --> xarray\n",
    "\n",
    "pr_sigtest=ss.ttest_ind(pr_nina_DJF, pr_other_DJF, axis=0, equal_var=False) # t-test for difference in mean\n",
    "pr_nina_pval=xr.DataArray(pr_sigtest.pvalue, \n",
    "                     coords={'lat':('lat',pr_nino.coords['lat'].data),\n",
    "                             'lon':('lon',pr_nino.coords['lon'].data)})  # numpy --> xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot la nina results\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "t_nina_DJF_composite.where(t_nina_pval<pval).plot(cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean temperature anomalies\\n during strong La Nina conditions (p < {pval})')\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "pr_nina_DJF_composite.where(pr_nina_pval<pval).plot(vmin=-2, vmax=2, extend='both',cmap='BrBG',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean precipitation anomalies\\n during strong La Nina conditions (p < {pval})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Do the spatial patterns of anomalous temperature and precipitation during strong EL Nino and strong La Nina look similar? What is similar and different?\n",
    "\n",
    "We've already calculated everything we need (our composites). We'll make one big plot to visually compare the following global patterns:\n",
    "- el nino temperature to la nina temperature\n",
    "- el nino precip to la nina precip\n",
    "- el nino temperature to el nino precip\n",
    "- la nina temperature to la nina precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,8))\n",
    "\n",
    "# t el nino\n",
    "ax=fig.add_subplot(221,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "t_nino_DJF_composite.where(t_nino_pval<pval).plot(vmin=-2, vmax=2, extend='both',cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean temperature anomalies\\n during strong El Nino conditions (p < {pval})')\n",
    "\n",
    "# pr el nino\n",
    "ax=fig.add_subplot(222,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "pr_nino_DJF_composite.where(pr_nino_pval<pval).plot(vmin=-2, vmax=2, extend='both',cmap='BrBG',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean precipitation anomalies\\n during strong El Nino conditions (p < {pval})')\n",
    "\n",
    "# t la nina\n",
    "ax=fig.add_subplot(223,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "t_nina_DJF_composite.where(t_nina_pval<pval).plot(vmin=-2, vmax=2, extend='both',cmap='RdBu_r',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean temperature anomalies\\n during strong La Nina conditions (p < {pval})')\n",
    "\n",
    "# pr la nina\n",
    "ax=fig.add_subplot(224,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "pr_nina_DJF_composite.where(pr_nina_pval<pval).plot(vmin=-2, vmax=2, extend='both',cmap='BrBG',cbar_kwargs={'shrink':0.9,'orientation':'horizontal','pad':0.05})\n",
    "plt.title(f'winter mean precipitation anomalies\\n during strong La Nina conditions (p < {pval})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Where, globally, is there a statistically significant correlation between the winter mean (DJF) nino3.4 index and winter mean temperature and precipitation?\n",
    "\n",
    "This is another statistical way to get at similar information that we found through composite analysis.\n",
    "\n",
    "In this case we'll compare the entire timeseries of DJF mean nino3.4 index values to the entire timeseries of DJF mean temperature and precipitation at each grid cell of our data.\n",
    "\n",
    "We usually detrend our data before this type of analysis.\n",
    "\n",
    "Why detrend?\n",
    "\n",
    "What we are really interested in here is how the ENSO cycle (3-7 years) of climate variability correlates to precipitation or temperature anomalies, globally. Often, especially with variables like temperature, the factor that explains the most variance in the data timeseries will be the long term trend, aka the \"climate change signal\". In this case, we don't care about the climate change signal, so we'll detrend our data by removing the long term linear trend from the nino3.4 index as well as from the timeseries or pr and temperature at each grid cell globally. \n",
    "\n",
    "There are plenty of other ways to detrend a timeseries that we won't cover here. How do you know which method to choose? Firstly, it depends on what data you are working with and what you are trying to ascertain from the correlation. My first step is to look at what's being done in similar scientific literature and, of course, test multiple methods to ensure the results are robust. \n",
    "\n",
    "So, does removing the linear trend mean that the remaining variance in our timeseries will only be due to the ENSO cycle? No! We can think about our timeseries as being composed of a trend, seasonality, and noise. After detrending, we'll be left with \"seasonality\" which will be a combination of all other forms/frequencies of cyclical natural climate variability (including ENSO) as well as the noise. For your own research, you may want to use a different detrend method or apply additional \"filters\" to further isolate the variance you are interested in. Or not! It's all very analysis-specific.\n",
    "\n",
    "We should note that after detrending, ENSO is the often the largest mode of global climate variability. If you are familiar with principal component analysis (PCA) or empirical orthogonal function analysis (EOF), for example, the first principle component of detrended deseasonalized global sea surface temperature reveals a clear ENSO spatial pattern when regressed back onto SST. We won't cover PCA/EOF analysis here but if you are interested in learning a little bit more here are some links \n",
    "- [an example of a PCA/EOF analysis on SST showing the first 6 modes of variability and providing a physical interpration of the spatial patterns](https://www.mbari.org/data/global-modes-of-sea-surface-temperature/)\n",
    "- [a warning about physical interpretations of PCA/EOF results](https://climatedataguide.ucar.edu/climate-tools/empirical-orthogonal-function-eof-analysis-and-rotated-eof-analysis)\n",
    "- see also the [Collection of Useful Links](#Collection-of-Useful-Links) section which has links to two climate science statistics textbooks\n",
    " \n",
    "Here, we'll keep it simple by using a linear detrend since we've already covered that in a previous workshop day and because it's a frequently used method in the scientific literature. As you know, some areas of the world have experienced greater climate changes than other areas of the world. We'll detrend each grid cell globally. For areas with large trends, larger adjustments to the timeseries will occur when we remove these trends. For areas with small trends, hardly any adjustment to the timeseries will occur when we remove these trends. Then when we do the timeseries correlation at each grid point we won't need to worry about whether the result at one grid cell vs another is influenced more by any prevalent long term linear trend, because we removed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating seasonal means\n",
    "\n",
    "# use resample: monthly data --> seasonal means \n",
    "# 'QS-DEC' means group months like DJF,MAM,JJA,SON and make the time label at the 'quarter start'\n",
    "# .thin({'time':4}) will pull out every 4th value in the resulting array (all the DJF values)\n",
    "# .isel(time=slice(1,-1)) will drop the first and last values of the resulting array (1947-12-01 and 2023-12-01), which were created with less than 3 months of data\n",
    "\n",
    "# nino3.4 index\n",
    "# this method of indexing [0::4][1:-1] works because this array is 1D\n",
    "ninoDJF=nino.resample(time='QS-DEC').mean('time')[0::4][1:-1] \n",
    "\n",
    "# precipitation anomalies\n",
    "pr_anomDJF=pr_anom.resample(time='QS-DEC').mean('time').thin({'time':4})\n",
    "pr_anomDJF=pr_anomDJF.isel(time=slice(1,-1))\n",
    "\n",
    "# temperature anomalies\n",
    "t_anomDJF=t_anom.resample(time='QS-DEC').mean('time').thin({'time':4})\n",
    "t_anomDJF=t_anomDJF.isel(time=slice(1,-1))\n",
    "\n",
    "# pr_anomDJF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll detrend using **[scipy.signal.detrend](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html)** to remove the linear trend from all our data\n",
    "\n",
    "scipy.signal.detrend can accept multidimensional array-like data as input (which includes xarray data arrays), but will always return a numpy array.\n",
    "\n",
    "Also, scipy.signal.detrend works by default on the last dimension. This default will work fine for our 1D data, but we'll need to tell the function to detrend the time dimension (axis 0) of our 3D data.\n",
    "\n",
    "One last thing to note is that scipy.signal.detrend can't handle nans, so for our 3D data we'll have to fill nans, detrend, then replace nans. This is ok here because the only places we have nan are ocean grid cells where all times are nan. If we had random nans in the timeseries for land points, we wouldn't use this fill-with-zero method. We'd have to take an extra step to address nans either by filling them with other data values (like the monthly climatological value) or eliminating grid cells with intermittent nans from the analysis all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrend \n",
    "\n",
    "# nino3.4 index\n",
    "result_numpy=detrend(ninoDJF)\n",
    "\n",
    "# convert numpy --> xarray\n",
    "d_ninoDJF=ninoDJF.copy()  # copy an xarray data array object (will give us all the labels)  \n",
    "d_ninoDJF[:]=result_numpy  # fill with the new detrended data values\n",
    "\n",
    "#-----------------------\n",
    "\n",
    "# precip\n",
    "# saving a mask of where the nans are \n",
    "# (which isn't actually necessary but can make it more obvious what we're doing in our code)\n",
    "nanmask=xr.where(np.isfinite(pr_anomDJF),0,1)  \n",
    "\n",
    "# fill nan --> 0\n",
    "d_pr_anomDJF=pr_anomDJF.copy().fillna(0)  \n",
    "\n",
    "# linear detrend in time\n",
    "result_numpy=detrend(d_pr_anomDJF,axis=0)  \n",
    "\n",
    "# convert numpy --> xarray\n",
    "d_pr_anomDJF[:,:,:]=result_numpy \n",
    "\n",
    "# put nans back\n",
    "d_pr_anomDJF=xr.where(nanmask,np.nan,d_pr_anomDJF)  \n",
    "\n",
    "#-----------------------\n",
    "\n",
    "# temperature\n",
    "# same process except this time demonstrating how to replace nans without creating a separate nanmask\n",
    "d_t_anomDJF=t_anomDJF.copy().fillna(0)    # nan --> 0 \n",
    "result_numpy=detrend(d_t_anomDJF,axis=0)  # linear detrend in time\n",
    "d_t_anomDJF[:,:,:]=result_numpy           # numpy --> xarray\n",
    "d_t_anomDJF=xr.where(np.isfinite(t_anomDJF),d_t_anomDJF,np.nan)  # put nans back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now prepared to correlate our 1D variable `d_ninoDJF` with each of our 3D variables `d_pr_anomDJF` and `d_t_anomDJF`. \n",
    "\n",
    "What function should we use? \n",
    "\n",
    "We want the correlation coefficient r as well as the p value. And, we want a function that performs fast on multidimensional data. \n",
    "\n",
    "We'll make a custom function that uses **[xarray.corr](https://docs.xarray.dev/en/stable/generated/xarray.corr.html)** for correlation and **[scipy.stats.t](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#scipy.stats.t)** for p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation with pvalues \n",
    "\n",
    "def corr_pval(x,y):\n",
    "    '''\n",
    "    function for correlating 1D xr.DataArray with dim 'time' to 3D xr.DataArray with dims 'time','lat','lon',\n",
    "    time labels of x and y must be identical\n",
    "    Parameters:\n",
    "    x: 1D xr.DataArray with dim 'time'\n",
    "    y: 3D xr.DataArray with dims 'time','lat','lon'\n",
    "    Returns:\n",
    "    (2-dimensional xr.DataArrays with dims 'lat', 'lon')\n",
    "    r: the correlation coefficient\n",
    "    p: the pvalue measure of statistical significance based on a two-tailed t-test\n",
    "    ''' \n",
    "    # correlation on time dim, ignore warning due to all nan cells\n",
    "    with warnings.catch_warnings(): \n",
    "        warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
    "        r=xr.corr(x,y,dim='time')\n",
    "    \n",
    "    # Compute t statistic and p-value\n",
    "    n=len(x.time)\n",
    "    tstat = r*np.sqrt(n-2)/np.sqrt(1-r**2)\n",
    "    p = ss.t.sf(abs(tstat), n-2)*2 # *2 for two-sided test\n",
    "    p = xr.DataArray(p, dims=r.dims, coords=r.coords) # numpy --> xarray\n",
    "    return r, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call our function\n",
    "r_tDJF,p_tDJF=corr_pval(d_ninoDJF,d_t_anomDJF)\n",
    "r_prDJF,p_prDJF=corr_pval(d_ninoDJF,d_pr_anomDJF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,4))\n",
    "\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.1)\n",
    "cbar_kwargs={'shrink':0.8}\n",
    "r_tDJF.where(p_tDJF<pval).plot(ax=ax,cmap='RdBu_r',cbar_kwargs=cbar_kwargs)\n",
    "plt.title(f'Temperature winter mean to nino3.4 correlation (p < {pval})')\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.1)\n",
    "cbar_kwargs={'shrink':0.8}\n",
    "r_prDJF.where(p_prDJF<pval).plot(ax=ax,cmap='BrBG',cbar_kwargs=cbar_kwargs)\n",
    "plt.title(f'Precip winter mean to nino3.4 correlation (p < {pval})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now instead of separate composites of strong El Nino and La Nina winters, we have the correlation coefficient of the entire winter timeseries. El Nino and La Nina patterns are not separated here.\n",
    "\n",
    "**What do the red and blue areas on the map of temperature correlations mean? What about the green and brown areas on the map of precipitation correlations?**\n",
    "\n",
    "On the temperature correlation map:\n",
    "- red areas mean a positive relationship between winter temperature and winter nino3.4 index --> generally warmer during el nino, cooler during la nina\n",
    "- blue areas mean a negative relationship between winter temps and winter nino3.4 index --> generally warmer during la nina, cooler during el nina\n",
    "\n",
    "On the precip correlation map:\n",
    "- green areas mean a positive relationship between winter precip and winter nino3.4 index --> generally wetter during el nino, drier during la nina\n",
    "- brown areas mean a negative relationship between winter precip and winter nino3.4 index --> generally wetter during la nina, drier during el nino\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) How much variance in winter mean (DJF) precipitation and temperature is explained (r squared) by the winter mean nino3.4 index at the following locations? \n",
    "- Pilot Station, AK\n",
    "- El Paso, TX\n",
    "- Medellin, Colombia\n",
    "- Johannesburg, South Africa\n",
    "- Davao, Philippines\n",
    "- Astana, Kazakhstan\n",
    "\n",
    "We've already calculated what we need in our variables `r_tDJF` and `r_prDJF`. We only need to select the data point closest to each location and square r.\n",
    "\n",
    "Let's take a look at where these places are by plotting a marker at each location on top of our correlation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places={'Pilot Station, AK':{'xy':(-162.8838,61.9369)},\n",
    "        'El Paso, TX':{'xy':(-106.4850,31.7619)},\n",
    "        'Medellin, Colombia':{'xy':(-75.5658,6.2476)},\n",
    "        'Johannesburg, South Africa':{'xy':(28.0337,-26.2056)},\n",
    "        'Astana, Kazakhstan':{'xy':(71.4272,51.1655)},\n",
    "        'Davao, Philippines':{'xy':(125.6110,7.0736)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,4))\n",
    "cbar_kwargs={'shrink':0.8}\n",
    "\n",
    "# temperature\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "# ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.1)\n",
    "r_tDJF.where(p_tDJF<pval).plot(ax=ax,cmap='RdBu_r',cbar_kwargs=cbar_kwargs)\n",
    "plt.title(f'Temperature winter mean to nino3.4 correlation (p < {pval})')\n",
    "\n",
    "for nested_dict in places.values():\n",
    "    x=nested_dict['xy'][0]\n",
    "    y=nested_dict['xy'][1]\n",
    "    ax.plot(x,y,marker='o',markersize=5,color='aqua')\n",
    "\n",
    "# precipitation\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.add_feature(cf.COASTLINE.with_scale(\"50m\"),lw=0.3)\n",
    "ax.add_feature(cf.BORDERS.with_scale(\"50m\"),lw=0.3)\n",
    "# ax.add_feature(cf.STATES.with_scale(\"50m\"),lw=0.1)\n",
    "r_prDJF.where(p_prDJF<pval).plot(ax=ax,cmap='BrBG',cbar_kwargs=cbar_kwargs)\n",
    "plt.title(f'Precip winter mean to nino3.4 correlation (p < {pval})')\n",
    "\n",
    "for nested_dict in places.values():\n",
    "    x=nested_dict['xy'][0]\n",
    "    y=nested_dict['xy'][1]\n",
    "    ax.plot(x,y,marker='o',markersize=5,color='aqua')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add % variance explained to the places dictionary and present it in a pretty pandas dataframe\n",
    "\n",
    "for key,nested_dict in places.items():      \n",
    "    lon=nested_dict['xy'][0]\n",
    "    lat=nested_dict['xy'][1]\n",
    "    r_t=(r_tDJF.sel(lat=lat, lon=lon, method='nearest')**2 *100).round(2)\n",
    "    r_pr=(r_prDJF.sel(lat=lat, lon=lon, method='nearest')**2 *100).round(2)\n",
    "\n",
    "    places[key]['t DJF % var explained by nino3.4']=r_t.item()\n",
    "    places[key]['pr DJF % var explained by nino3.4']=r_pr.item()\n",
    "\n",
    "pd.DataFrame.from_dict(places,orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a netcdf file\n",
    "\n",
    "Showing how to write a netcdf file of your results. We'll write the variable `r_tDJF` to a file.\n",
    "\n",
    "First, let's look at the labels/metadata attached to `r_tDJF`. We'll want to add additional variable and file metadata before we write the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tDJF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add variable name\n",
    "varname='r'\n",
    "r_tDJF.name=varname\n",
    "\n",
    "# add variable attributes\n",
    "var_attrs={'standard_name':varname,\n",
    "          'units':'unitless',\n",
    "           '_FillValue':np.nan,\n",
    "          'description':'correlation coefficient of detrended winter mean (DJF) Nino3.4 Index with detrended winter mean (DJF) temperature anomalies'}\n",
    "\n",
    "r_tDJF=r_tDJF.assign_attrs(var_attrs)\n",
    "\n",
    "# add lat coordinate attributes\n",
    "lat_attrs={'standard_name':'latitude',\n",
    "          'units':'degees_north',\n",
    "          'axis':'Y'}\n",
    "lon_attrs={'standard_name':'longitude',\n",
    "          'units':'degees_east',\n",
    "          'axis':'X'}\n",
    "\n",
    "r_tDJF['lat']=r_tDJF.lat.assign_attrs(lat_attrs)\n",
    "r_tDJF['lon']=r_tDJF.lon.assign_attrs(lon_attrs)\n",
    "\n",
    "# change data type\n",
    "# we don't need to save this variable with double precision\n",
    "r_tDJF=r_tDJF.astype('float32')\n",
    "r_tDJF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataset\n",
    "ds_out=r_tDJF.to_dataset()\n",
    "\n",
    "# assign file attributes\n",
    "file_attrs={'temperature_data_source':'https://berkeley-earth-temperature.s3.us-west-1.amazonaws.com/Global/Gridded/Complete_TAVG_LatLong1.nc',\n",
    "           'nino_index_data_source':'https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/nino34.long.anom.data',\n",
    "           'code_source':'https://github.com/kerriegeil/MSU_py_training/blob/main/learn_by_doing/enso_analysis.ipynb'}\n",
    "\n",
    "ds_out.attrs=file_attrs\n",
    "\n",
    "# maybe you'd also want to assign a spatial reference\n",
    "ds_out.rio.write_crs(\"epsg:4326\", inplace=True)  # rioxarray.write_crs()\n",
    "ds_out[varname].attrs['coordinates']='spatial_ref'\n",
    "\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "outfile='some_descriptive_filename.nc'\n",
    "ds_out.to_netcdf(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our new netcdf file to make sure everything worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset(outfile)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.r.plot()\n",
    "plt.title('r from our new netcdf file')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyworkshop",
   "language": "python",
   "name": "pyworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "affcdfbef34b243274bba1a77328c47db46b2eb84771471f779184b380d9e9a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
